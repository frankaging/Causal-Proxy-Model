{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Baseline for CEBaB\n",
    "\n",
    "This is one interesting baseline. For each model architecture, we basically take a randomly initialized model and evaluate CEBaB score. This is different from the `RandomExplainer` mentioned in the paper. Here, we actually have model with random weights. \n",
    "\n",
    "This script simply randomly initialize different models and save to disk for evaluation.\n",
    "\n",
    "**Note**: For random initialized model, there are two ways: (1) taking the pretrained weights which is really bad at classifying things. (2) randomly initialized model. For the `LSTM` model, it is a little tricky, but I don't think there is a much difference in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following blocks will run CEBaB benchmark in\n",
    "all the combinations of the following conditions.\n",
    "\"\"\"\n",
    "grid = {\n",
    "    \"seed\": [42, 66, 77, 88, 99],\n",
    "    \"class_num\": [2, 3, 5],\n",
    "    \"model_arch\" : [\"bert-base-uncased\", \"roberta-base\", \"gpt2\", \"lstm\"]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first type where we directly import pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(permutations_dicts)):\n",
    "    seed=permutations_dicts[i][\"seed\"]\n",
    "    class_num=permutations_dicts[i][\"class_num\"]\n",
    "    model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "    h_dim = 75 if \"lstm\" in model_arch else 192\n",
    "    if model_arch == \"bert-base-uncased\":\n",
    "        model_path = \"BERT-pretrain-results\"\n",
    "    elif model_arch == \"roberta-base\":\n",
    "        model_path = \"RoBERTa-pretrain-results\"\n",
    "    elif model_arch == \"gpt2\":\n",
    "        model_path = \"gpt2-pretrain-results\"\n",
    "    elif model_arch == \"lstm\":\n",
    "        model_path = \"lstm-pretrain-results\"\n",
    "        \n",
    "    output_dir = f\"../proxy_training_results/{model_path}/cebab.train.train\"\\\n",
    "                 f\".alpha.1.0.beta.0.0.gemma.0.0\"\\\n",
    "                 f\".dim.{h_dim}\"\\\n",
    "                 f\".hightype.{model_arch}.Proxy.CEBaB.sa.{class_num}-class.exclusive.mode.align\"\\\n",
    "                 f\".cls.dropout.0.1.enc.dropout.0.1.seed_{seed}\"\n",
    "    print(\"outputting to: \", output_dir)\n",
    "    \n",
    "    config_name = None\n",
    "    tokenizer_name = None\n",
    "    if model_arch == \"lstm\":\n",
    "        config_name = \"bert-base-uncased\"\n",
    "        tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        config_name if config_name else model_arch,\n",
    "        num_labels=class_num,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name if tokenizer_name else model_arch,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "    if \"lstm\" not in model_arch:\n",
    "        if \"bert-base-uncased\" in model_arch:\n",
    "            model_serving_module = BertForNonlinearSequenceClassification\n",
    "        elif \"gpt2\" in model_arch:\n",
    "            model_serving_module = GPT2ForNonlinearSequenceClassification\n",
    "        elif \"roberta\" in model_arch:\n",
    "            model_serving_module = RobertaForNonlinearSequenceClassification\n",
    "        model = model_serving_module.from_pretrained(\n",
    "            model_arch,\n",
    "            from_tf=False,\n",
    "            config=config,\n",
    "            cache_dir=\"../huggingface_cache/\",\n",
    "        )\n",
    "        # some post-editing for customized models.\n",
    "        if model_arch == \"gpt2\":\n",
    "            # Define a padding token\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    elif \"lstm\" in model_arch:\n",
    "        config.update_embeddings=False\n",
    "        config.bidirectional=True\n",
    "        config.num_hidden_layers=1\n",
    "        config.hidden_size=300\n",
    "        model = LSTMForNonLinearSequenceClassification(\n",
    "            config=config,\n",
    "        )\n",
    "        # load the preloaded embedding file.\n",
    "        fasttext_embeddings = torch.load(\"../eval_pipeline/customized_models/lstm/embeddings.bin\")\n",
    "        model.lstm.embeddings.word_embeddings.weight.data = fasttext_embeddings\n",
    "    model.save_pretrained(\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the second case where all weights are randomly initialized! They are not pretrained anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(permutations_dicts)):\n",
    "    seed=permutations_dicts[i][\"seed\"]\n",
    "    class_num=permutations_dicts[i][\"class_num\"]\n",
    "    model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "    h_dim = 75 if \"lstm\" in model_arch else 192\n",
    "    if model_arch == \"bert-base-uncased\":\n",
    "        model_path = \"BERT-random-results\"\n",
    "    elif model_arch == \"roberta-base\":\n",
    "        model_path = \"RoBERTa-random-results\"\n",
    "    elif model_arch == \"gpt2\":\n",
    "        model_path = \"gpt2-random-results\"\n",
    "    elif model_arch == \"lstm\":\n",
    "        model_path = \"lstm-random-results\"\n",
    "        \n",
    "    output_dir = f\"../proxy_training_results/{model_path}/cebab.train.train\"\\\n",
    "                 f\".alpha.1.0.beta.0.0.gemma.0.0\"\\\n",
    "                 f\".dim.{h_dim}\"\\\n",
    "                 f\".hightype.{model_arch}.Proxy.CEBaB.sa.{class_num}-class.exclusive.mode.align\"\\\n",
    "                 f\".cls.dropout.0.1.enc.dropout.0.1.seed_{seed}\"\n",
    "    print(\"outputting to: \", output_dir)\n",
    "    \n",
    "    config_name = None\n",
    "    tokenizer_name = None\n",
    "    if model_arch == \"lstm\":\n",
    "        config_name = \"bert-base-uncased\"\n",
    "        tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        config_name if config_name else model_arch,\n",
    "        num_labels=class_num,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name if tokenizer_name else model_arch,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "    if \"lstm\" not in model_arch:\n",
    "        if \"bert-base-uncased\" in model_arch:\n",
    "            model_serving_module = BertForNonlinearSequenceClassification\n",
    "        elif \"gpt2\" in model_arch:\n",
    "            model_serving_module = GPT2ForNonlinearSequenceClassification\n",
    "        elif \"roberta\" in model_arch:\n",
    "            model_serving_module = RobertaForNonlinearSequenceClassification\n",
    "        model = model_serving_module(\n",
    "            config=config,\n",
    "        )\n",
    "        # some post-editing for customized models.\n",
    "        if model_arch == \"gpt2\":\n",
    "            # Define a padding token\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    elif \"lstm\" in model_arch:\n",
    "        config.update_embeddings=False\n",
    "        config.bidirectional=True\n",
    "        config.num_hidden_layers=1\n",
    "        config.hidden_size=300\n",
    "        model = LSTMForNonLinearSequenceClassification(\n",
    "            config=config,\n",
    "        )\n",
    "        # load the preloaded embedding file.\n",
    "        fasttext_embeddings = torch.load(\"../eval_pipeline/customized_models/lstm/embeddings.bin\")\n",
    "        model.lstm.embeddings.word_embeddings.weight.data = nn.Embedding(\n",
    "            fasttext_embeddings.shape[0], fasttext_embeddings.shape[1]\n",
    "        ).weight.data\n",
    "    model.save_pretrained(\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
