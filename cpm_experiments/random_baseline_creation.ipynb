{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Baseline for CEBaB\n",
    "\n",
    "This is one interesting baseline. For each model architecture, we basically take a randomly initialized model and evaluate CEBaB score. This is different from the `RandomExplainer` mentioned in the paper. Here, we actually have model with random weights. \n",
    "\n",
    "This script simply randomly initialize different models and save to disk for evaluation.\n",
    "\n",
    "**Note**: For random initialized model, there are two ways: (1) taking the pretrained weights which is really bad at classifying things. (2) randomly initialized model. For the `LSTM` model, it is a little tricky, but I don't think there is a much difference in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following blocks will run CEBaB benchmark in\n",
    "all the combinations of the following conditions.\n",
    "\"\"\"\n",
    "grid = {\n",
    "    \"seed\": [42, 66, 77],\n",
    "    \"class_num\": [5],\n",
    "    \"model_arch\" : [\"bert-base-uncased\"]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to:  ../proxy_training_results/BERT-baseline-random/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-05.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_42/\n",
      "outputting to:  ../proxy_training_results/BERT-baseline-random/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-05.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_66/\n",
      "outputting to:  ../proxy_training_results/BERT-baseline-random/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-05.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_77/\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(permutations_dicts)):\n",
    "    seed=permutations_dicts[i][\"seed\"]\n",
    "    class_num=permutations_dicts[i][\"class_num\"]\n",
    "    model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "    if model_arch == \"bert-base-uncased\":\n",
    "        model_path = \"BERT-baseline-random\"\n",
    "        interchange_layer = 10\n",
    "        h_dim = 192\n",
    "    elif model_arch == \"roberta-base\":\n",
    "        model_path = \"RoBERTa-baseline-random\"\n",
    "    elif model_arch == \"gpt2\":\n",
    "        model_path = \"gpt2-baseline-random\"\n",
    "    elif model_arch == \"lstm\":\n",
    "        model_path = \"lstm-baseline-random\"\n",
    "\n",
    "    output_dir = f'../proxy_training_results/{model_path}/'\\\n",
    "                 f'cebab.alpha.0.0.beta.0.0.gemma.0.0.'\\\n",
    "                 f'lr.8e-05.dim.{h_dim}.hightype.{model_arch}.'\\\n",
    "                 f'CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.'\\\n",
    "                 f'approximate.k.0.int.layer.{interchange_layer}.'\\\n",
    "                 f'seed_{seed}/'\n",
    "    print(\"outputting to: \", output_dir)\n",
    "    \n",
    "    config_name = None\n",
    "    tokenizer_name = None\n",
    "    if model_arch == \"lstm\":\n",
    "        config_name = \"bert-base-uncased\"\n",
    "        tokenizer_name = \"bert-base-uncased\"\n",
    "        \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        config_name if config_name else model_arch,\n",
    "        num_labels=class_num,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "    )\n",
    "    config.intervention_h_dim = h_dim\n",
    "    config.interchange_hidden_layer = interchange_layer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name if tokenizer_name else model_arch,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "    if \"bert-base-uncased\" in model_arch:\n",
    "        model_serving_module = IITBERTForSequenceClassification\n",
    "    elif \"gpt2\" in model_arch:\n",
    "        model_serving_module = IITGPT2ForSequenceClassification\n",
    "    elif \"roberta\" in model_arch:\n",
    "        model_serving_module = IITRobertaForSequenceClassification\n",
    "    elif \"lstm\" in model_arch:\n",
    "        model_serving_module = IITLSTMForSequenceClassification\n",
    "        config.update_embeddings=False\n",
    "        config.bidirectional=True\n",
    "        config.num_hidden_layers=1\n",
    "        config.hidden_size=300\n",
    "    model = model_serving_module(\n",
    "        config=config,\n",
    "    )\n",
    "    if \"lstm\" in model_arch:\n",
    "        # load the preloaded embedding file.\n",
    "        fasttext_embeddings = torch.load(\"../eval_pipeline/customized_models/lstm/embeddings.bin\")\n",
    "        model.lstm.embeddings.word_embeddings.weight.data = nn.Embedding(\n",
    "            fasttext_embeddings.shape[0], fasttext_embeddings.shape[1]\n",
    "        ).weight.data\n",
    "    # some post-editing for customized models.\n",
    "    if model_arch == \"gpt2\":\n",
    "        # Define a padding token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.save_pretrained(\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black-box Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to:  ../proxy_training_results/BERT-baseline-blackbox/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-5.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_42/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at ../saved_models/bert-base-uncased.opentable.CEBaB.sa.5-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.out_proj.weight', 'multitask_classifier.dense.bias', 'multitask_classifier.dense.weight', 'multitask_classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to:  ../proxy_training_results/BERT-baseline-blackbox/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-5.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_66/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at ../saved_models/bert-base-uncased.opentable.CEBaB.sa.5-class.exclusive.seed_66 and are newly initialized: ['multitask_classifier.out_proj.weight', 'multitask_classifier.dense.bias', 'multitask_classifier.dense.weight', 'multitask_classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to:  ../proxy_training_results/BERT-baseline-blackbox/cebab.alpha.0.0.beta.0.0.gemma.0.0.lr.8e-5.dim.192.hightype.bert-base-uncased.CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.approximate.k.0.int.layer.10.seed_77/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at ../saved_models/bert-base-uncased.opentable.CEBaB.sa.5-class.exclusive.seed_77 and are newly initialized: ['multitask_classifier.out_proj.weight', 'multitask_classifier.dense.bias', 'multitask_classifier.dense.weight', 'multitask_classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(permutations_dicts)):\n",
    "    seed=permutations_dicts[i][\"seed\"]\n",
    "    class_num=permutations_dicts[i][\"class_num\"]\n",
    "    model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "    h_dim = 75 if \"lstm\" in model_arch else 192\n",
    "    if model_arch == \"bert-base-uncased\":\n",
    "        model_path = \"BERT-baseline-blackbox\"\n",
    "        interchange_layer = 10\n",
    "        h_dim = 192\n",
    "    elif model_arch == \"roberta-base\":\n",
    "        model_path = \"RoBERTa-baseline-blackbox\"\n",
    "    elif model_arch == \"gpt2\":\n",
    "        model_path = \"gpt2-baseline-blackbox\"\n",
    "    elif model_arch == \"lstm\":\n",
    "        model_path = \"lstm-baseline-blackbox\"\n",
    "        \n",
    "    blackbox_model_path = f'../saved_models/{model_arch}.opentable.CEBaB.sa.'\\\n",
    "                          f'{class_num}-class.exclusive.seed_{seed}'\n",
    "    output_dir = f'../proxy_training_results/{model_path}/'\\\n",
    "                 f'cebab.alpha.0.0.beta.0.0.gemma.0.0.'\\\n",
    "                 f'lr.8e-5.dim.{h_dim}.hightype.{model_arch}.'\\\n",
    "                 f'CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.'\\\n",
    "                 f'approximate.k.0.int.layer.{interchange_layer}.'\\\n",
    "                 f'seed_{seed}/'\n",
    "    \n",
    "    print(\"outputting to: \", output_dir)\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        blackbox_model_path,\n",
    "        num_labels=class_num,\n",
    "        cache_dir=\"../huggingface_cache/\",\n",
    "    )\n",
    "    config.intervention_h_dim = h_dim\n",
    "    config.interchange_hidden_layer = interchange_layer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        blackbox_model_path,\n",
    "        cache_dir=\"../../huggingface_cache/\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "\n",
    "    if \"bert-base-uncased\" in model_arch:\n",
    "        model_serving_module = IITBERTForSequenceClassification\n",
    "    elif \"gpt2\" in model_arch:\n",
    "        model_serving_module = IITGPT2ForSequenceClassification\n",
    "    elif \"roberta\" in model_arch:\n",
    "        model_serving_module = IITRobertaForSequenceClassification\n",
    "    elif \"lstm\" in model_arch:\n",
    "        model_serving_module = IITLSTMForSequenceClassification\n",
    "        config.update_embeddings=False\n",
    "        config.bidirectional=True\n",
    "        config.num_hidden_layers=1\n",
    "        config.hidden_size=300\n",
    "    model = model_serving_module.from_pretrained(\n",
    "        blackbox_model_path,\n",
    "        config=config,\n",
    "        cache_dir=\"../../huggingface_cache\"\n",
    "    )\n",
    "    # some post-editing for customized models.\n",
    "    if model_arch == \"gpt2\":\n",
    "        # Define a padding token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    model.save_pretrained(\n",
    "        output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
