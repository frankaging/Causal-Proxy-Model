{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Explanations of Causal Proxy Model\n",
    "\n",
    "This notebook generates gradient-based visualizations to show how different representations within a CPM model capture the different aspects that they are trained to represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "Loads modules, CEBaB data, and Static Proxy Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *\n",
    "from IPython.display import display, HTML\n",
    "\"\"\"\n",
    "For evaluate, we use a single random seed, as\n",
    "the models are trained with 5 different seeds\n",
    "already.\n",
    "\"\"\"\n",
    "_ = random.seed(123)\n",
    "_ = np.random.seed(123)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CEBaB data and model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following blocks will run CEBaB benchmark in\n",
    "all the combinations of the following conditions.\n",
    "\"\"\"\n",
    "grid = {\n",
    "    \"eval_split\": [\"test\"],\n",
    "    # dev,test\n",
    "    \"control\": [\"ks\"],\n",
    "    # baseline-random,baseline-blackbox,hdims,layers,ks,approximate,ablation\n",
    "    \"seed\": [42],\n",
    "    # 42, 66, 77\n",
    "    \"h_dim\": [192],\n",
    "    # 1,16,64,128,192\n",
    "    # 1,16,64,75\n",
    "    \"interchange_layer\" : [10],\n",
    "    # 0,1; 2,4,6,8,10,12\n",
    "    \"class_num\": [5],\n",
    "    \"k\" : [19684], \n",
    "    # 0;10,100,500,1000,3000,6000,9848,19684\n",
    "    \"alpha\" : [1.0],\n",
    "    # 0.0,1.0\n",
    "    \"beta\" : [1.0],\n",
    "    # 0.0,1.0\n",
    "    \"gemma\" : [3.0],\n",
    "    # 0.0,3.0\n",
    "    \"model_arch\" : [\"bert-base-uncased\"],\n",
    "    # lstm, bert-base-uncased, roberta-base, gpt2\n",
    "    \"lr\" : [\"8e-05\"],\n",
    "    # 8e-05; 0.001\n",
    "    \"counterfactual_type\" : [\"true\"]\n",
    "    # approximate,true\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "device = 'cuda:2'\n",
    "batch_size = 32\n",
    "\n",
    "if grid[\"control\"][0] == \"hdims\" or grid[\"control\"][0] == \"layers\":\n",
    "    assert grid[\"eval_split\"][0] == \"dev\"\n",
    "else:\n",
    "    assert grid[\"eval_split\"][0] == \"test\"\n",
    "    \n",
    "aspect_label_encode = {\n",
    "    \"Negative\":0,\n",
    "    \"Positive\":1,\n",
    "    \"unknown\":2,\n",
    "    \"no majority\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "eval_split=permutations_dicts[i][\"eval_split\"]\n",
    "seed=permutations_dicts[i][\"seed\"]\n",
    "class_num=permutations_dicts[i][\"class_num\"]\n",
    "alpha=permutations_dicts[i][\"alpha\"]\n",
    "beta=permutations_dicts[i][\"beta\"]\n",
    "gemma=permutations_dicts[i][\"gemma\"]\n",
    "h_dim=permutations_dicts[i][\"h_dim\"]\n",
    "dataset_type = f'{class_num}-way'\n",
    "control=permutations_dicts[i][\"control\"]\n",
    "model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "k=permutations_dicts[i][\"k\"]\n",
    "interchange_layer=permutations_dicts[i][\"interchange_layer\"]\n",
    "lr=permutations_dicts[i][\"lr\"]\n",
    "counterfactual_type=permutations_dicts[i][\"counterfactual_type\"]\n",
    "\n",
    "if model_arch == \"bert-base-uncased\":\n",
    "    model_path = \"BERT\"\n",
    "    model_module = BERTForCEBaB\n",
    "    explainer_module = CausalProxyModelForBERT\n",
    "elif model_arch == \"roberta-base\":\n",
    "    model_path = \"RoBERTa\" \n",
    "    model_module = RoBERTaForCEBaB\n",
    "    explainer_module = CausalProxyModelForRoBERTa\n",
    "elif model_arch == \"gpt2\":\n",
    "    model_path = \"gpt2\"\n",
    "    model_module = GPT2ForCEBaB\n",
    "    explainer_module = CausalProxyModelForGPT2\n",
    "elif model_arch == \"lstm\":\n",
    "    model_path = \"lstm\"\n",
    "    model_module = LSTMForCEBaB\n",
    "    explainer_module = CausalProxyModelForLSTM\n",
    "model_path += f\"-{control}\"\n",
    "grid_conditions=(\n",
    "    (\"eval_split\", eval_split),\n",
    "    (\"control\", control),\n",
    "    (\"seed\", seed),\n",
    "    (\"h_dim\", h_dim),\n",
    "    (\"interchange_layer\", interchange_layer),\n",
    "    (\"class_num\", class_num),\n",
    "    (\"k\", k),\n",
    "    (\"alpha\", alpha),\n",
    "    (\"beta\", beta),\n",
    "    (\"gemma\", gemma),\n",
    "    (\"model_arch\", model_arch),\n",
    "    (\"lr\", lr),\n",
    "    (\"counterfactual_type\", counterfactual_type)\n",
    ")\n",
    "print(\"Running for this setting: \", grid_conditions)\n",
    "\n",
    "blackbox_model_path = f'CEBaB/{model_arch}.CEBaB.sa.'\\\n",
    "                      f'{class_num}-class.exclusive.seed_{seed}'\n",
    "cpm_model_path = f'../proxy_training_results/{model_path}/'\\\n",
    "                 f'cebab.alpha.{alpha}.beta.{beta}.gemma.{gemma}.'\\\n",
    "                 f'lr.{lr}.dim.{h_dim}.hightype.{model_arch}.'\\\n",
    "                 f'CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.'\\\n",
    "                 f'{counterfactual_type}.k.{k}.int.layer.{interchange_layer}.'\\\n",
    "                 f'seed_{seed}/'\n",
    "\n",
    "# load data from HF\n",
    "cebab = datasets.load_dataset(\n",
    "    'CEBaB/CEBaB', use_auth_token=True,\n",
    "    cache_dir=\"../train_cache/\"\n",
    ")\n",
    "\n",
    "train, dev, test = preprocess_hf_dataset_inclusive(\n",
    "    cebab, verbose=1, dataset_type=dataset_type\n",
    ")\n",
    "\n",
    "eval_dataset = dev if eval_split == 'dev' else test\n",
    "\n",
    "tf_model = model_module(\n",
    "    blackbox_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "explainer = explainer_module(\n",
    "    blackbox_model_path,\n",
    "    cpm_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size,\n",
    "    intervention_h_dim=h_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Saliency Map\n",
    "\n",
    "The code below sets up a pipeline for computing and visualizing saliency maps for the causal proxy model, and the control blackbox model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - x.max())\n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, explanator):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence using the `explanator` tokenizer\n",
    "    \"\"\"\n",
    "    explanator.blackbox_model.eval()\n",
    "    explanator.cpm_model.model.eval()\n",
    "    is_split_into_words = isinstance(sentence, List)\n",
    "    # Original ratings.\n",
    "    x = explanator.tokenizer([sentence], padding=True, truncation=True, return_tensors='pt', is_split_into_words=is_split_into_words)\n",
    "    x_batch = {k: v.to(explanator.device) for k, v in x.items()}\n",
    "    return x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(\n",
    "    x, \n",
    "    key_fn=lambda x: x\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns maximum of n-dimensional array `x` at last dimension.\n",
    "\n",
    "    Parameters \n",
    "    -----------\n",
    "    x : numpy narray\n",
    "    key_fn : single-variable function, default is identity\n",
    "        Function by which to sort input in order to choose maximum value. By default,\n",
    "        no key function is used.\n",
    "    \"\"\"\n",
    "    index = np.argmax(key_fn(x), axis=-1)\n",
    "    shp = np.array(x.shape)\n",
    "    dim_index = [np.arange(i) for i in shp[:-1]]\n",
    "    dim_index.append(index)\n",
    "    return x[tuple(dim_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_word(word, weight, is_significant, is_pos, pos_color='224,108,120', neg_color='88,117,220', max_weight=0.8):\n",
    "    \"\"\"\n",
    "    Highlights a word using HTML based on its weight\n",
    "    \"\"\"\n",
    "    if not is_significant:\n",
    "        return word\n",
    "    color = pos_color if is_pos else neg_color\n",
    "    return f\"\"\"<span style=\"background-color:rgba({color},{str(abs(weight) / max_weight)})\">{word}</span>\"\"\"\n",
    "\n",
    "def display_colored_text(plot_df, n_color=10):\n",
    "    \"\"\"\n",
    "    Displays sentence as colored words \n",
    "    NOTE: needs some calibration\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    for model in plot_df['model'].unique():\n",
    "        model_df = plot_df[plot_df['model'] == model].copy()\n",
    "        most_significant_words = model_df.sort_values(by='weight', key=np.abs, ascending=False).iloc[:n_color]['word']\n",
    "        model_df['is_pos'] = model_df['weight'] > 0\n",
    "        model_df['is_significant'] = model_df['word'].isin(most_significant_words)\n",
    "        to_plot = model_df.apply(\n",
    "            lambda r: highlight_word(r['word'], r['weight'], r['is_significant'], r['is_pos'], max_weight=model_df['weight'].abs().max()),\n",
    "            axis=1\n",
    "        )\n",
    "        colored_text = ' '.join(list(to_plot.values))\n",
    "        title = f'Saliency for {model}:'\n",
    "        result += f'<p>{title}</p><p>{colored_text}</p>'\n",
    "    result = f\"\"\"<div style=\"background-color:#FFFFFF;color:black\">{result}</div>\"\"\"\n",
    "    display(HTML(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bar_plot(plot_df):\n",
    "    \"\"\"\n",
    "    Helper function to plot saliency map for `explain_interventions_with_gradients`.\n",
    "    \"\"\"\n",
    "    g = sns.catplot(\n",
    "        data=plot_df,\n",
    "        x='word',\n",
    "        y='weight',\n",
    "        col='model',\n",
    "        kind='bar'\n",
    "    )\n",
    "    axes = g.axes[0]\n",
    "    for ax in axes:\n",
    "        _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Functions\n",
    "\n",
    "The functions below each compute the gradients across the lowest layer of the provided model, which corresponds to the inputted token embeddings. These gradients are then used to compute and visualize the saliency of the model with respect to the specified aspect.\n",
    "\n",
    "The functions are as follows:\n",
    " - `compute_saliency_of_logits`: takes the gradient of the input layer with respect to the logit of the specified aspect; if no aspect is specified (for control), then the gradient is taken with respect to the overall sentiment logit. NOTE: this function will not work with the blackbox model, since it does not compute aspect-specific logits.\n",
    " - `compute_saliency_of_final_layer`: takes the gradient of the input layer with respect to the slice of the final layer that represents the specified aspect; if no aspect is specified (for control), then the gradient is taken with respect to the entire final layer.\n",
    " - `compute_weighted_saliency`: takes the gradient of the input layer with respect to the slice of the final layer that represents the specified aspect, but weighted by the gradient from the final logit to the final layer; if no aspect is specified, then the gradient is taken with respect to the entire final layer.\n",
    " - `compute_guided_saliency`: takes the gradient of the input layer with respect to logit for overall sentiment, but only guided through the slice of the final layer that represents the specified aspect; if no aspect is specified (for control), then the gradient is guided through the entire final layer (should be the same as taking the gradient of the input with respect to the final overall logit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_of_logits(model, x, explanator, aspect=None):\n",
    "    \"\"\"\n",
    "    Applies a saliency map on the logits of the output of the `explanator` \n",
    "    model when run on a single input sentence. It uses the tokens produced by\n",
    "    the `explanator` tokenizer to determine which parts of the input had the\n",
    "    most influence on the chosen logit.\n",
    "\n",
    "    aspect : int\n",
    "        Chooses which output to focus on, from the following:\n",
    "        0 - ambiance\n",
    "        1 - food\n",
    "        2 - noise\n",
    "        3 - service\n",
    "        None - overall review\n",
    "    \"\"\"\n",
    "    logit_index = aspect + 1 if aspect is not None else 0\n",
    "\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    outputs = model(\n",
    "        **x,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "    outputs.hidden_states[0].retain_grad()\n",
    "\n",
    "    classification = outputs.logits[logit_index].argmax()\n",
    "    output_max = outputs.logits[logit_index][0, classification]\n",
    "    output_max.backward()\n",
    "\n",
    "    return outputs.hidden_states[0].grad.data.clone().detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_of_final_layer(model, x, explanator, aspect=None):\n",
    "    \"\"\"\n",
    "    Computes gradient-based saliency of how the input features affect \n",
    "    the computation of the final layer of the CPM model, which is used to\n",
    "    predict the overall sentiment of a sentence.\n",
    "\n",
    "    model\n",
    "        model to explain (either cpm_model.model or blackbox_model)\n",
    "    x\n",
    "        tokenized sentence to explain\n",
    "    explanator\n",
    "        CPM model\n",
    "    aspect : int\n",
    "        Different aspects correspond to different slices of the \n",
    "        final model layer. The options are:\n",
    "        0 - ambiance\n",
    "        1 - food\n",
    "        2 - noise\n",
    "        3 - service\n",
    "        None - all of the above (for control)\n",
    "    \"\"\"\n",
    "    # zero gradients\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # compute output\n",
    "    outputs = model(\n",
    "        **x,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    outputs.hidden_states[0].retain_grad()\n",
    "\n",
    "    # get slice of final layer that corresponds to aspect provided\n",
    "    if aspect is not None:\n",
    "        start_idx = aspect*explanator.intervention_h_dim\n",
    "        end_idx = (aspect+1)*explanator.intervention_h_dim\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = outputs.hidden_states[-1].size(2)\n",
    "    \n",
    "    # run gradient backwards for the final layer,\n",
    "    # only computing the gradients for the range of the specified aspect\n",
    "    output_grad = outputs.hidden_states[-1]\n",
    "    mask = torch.zeros_like(output_grad)\n",
    "    mask[:, 0, start_idx:end_idx] = 1\n",
    "    output_grad.backward(gradient=mask)\n",
    "\n",
    "    # return gradient at input layer\n",
    "    return outputs.hidden_states[0].grad.data.clone().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_saliency(model, x, explanator, aspect=None):\n",
    "    \"\"\"\n",
    "    Computes the gradient-based saliency of the output logits,\n",
    "    through the specified slice of the final layer of the CPM model.\n",
    "    Corresponds to\n",
    "    dy/d(aspect representation) * d(aspect representation)/dx\n",
    "\n",
    "    model\n",
    "        model to explain (either cpm_model.model or blackbox_model)\n",
    "    x\n",
    "        tokenized sentence to explain\n",
    "    explanator\n",
    "        CPM model\n",
    "    aspect : int\n",
    "        Different aspects correspond to different slices of the \n",
    "        final model layer. The options are:\n",
    "        0 - ambiance\n",
    "        1 - food\n",
    "        2 - noise\n",
    "        3 - service\n",
    "        None - all of the above (for control)\n",
    "    \"\"\"\n",
    "    # zero gradients\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # compute output\n",
    "    outputs = model(\n",
    "        **x,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "    # get aspect representation\n",
    "    if aspect is not None:\n",
    "        start_idx = aspect*explanator.intervention_h_dim\n",
    "        end_idx = (aspect+1)*explanator.intervention_h_dim\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = outputs.hidden_states[-1].size(2)\n",
    "\n",
    "    first_layer = outputs.hidden_states[0]\n",
    "    last_layer = outputs.hidden_states[-1]\n",
    "\n",
    "    first_layer.retain_grad()\n",
    "\n",
    "    # compute backwards gradient from final layer to input layer\n",
    "    mask = torch.zeros_like(last_layer)\n",
    "    mask[:, 0, start_idx:end_idx] = 1\n",
    "    last_layer.backward(gradient=mask, retain_graph=True)\n",
    "\n",
    "    first_layer_gradient = first_layer.grad.data.clone().detach()\n",
    "\n",
    "\n",
    "    # reset gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    last_layer.retain_grad()\n",
    "\n",
    "    # compute backwards gradient from output logits to final layer\n",
    "    logits = outputs.logits[0] if isinstance(outputs.logits, tuple) else outputs.logits\n",
    "    classification = logits.argmax()\n",
    "    output_max = logits[0, classification]\n",
    "    output_max.backward(retain_graph=True)\n",
    "\n",
    "    last_layer_gradient = last_layer.grad.data.clone().detach()\n",
    "\n",
    "    # multiply gradient at input level with gradient of CLS vector at final layer, \n",
    "    # since only this slice is used in computing the output logits\n",
    "    weighted_gradient = first_layer_gradient * last_layer_gradient[:, 0, :]\n",
    "\n",
    "    return weighted_gradient.clone().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_guided_saliency(model, x, explanator, aspect=None):\n",
    "    \"\"\"\n",
    "    Computes the gradient-based saliency of the output logits,\n",
    "    through the specified slice of the final layer of the CPM model.\n",
    "    Corresponds to\n",
    "    dy/d(aspect representation) * d(aspect representation)/dx\n",
    "\n",
    "    model\n",
    "        model to explain (either cpm_model.model or blackbox_model)\n",
    "    x\n",
    "        tokenized sentence to explain\n",
    "    explanator\n",
    "        CPM model\n",
    "    aspect : int\n",
    "        Different aspects correspond to different slices of the \n",
    "        final model layer. The options are:\n",
    "        0 - ambiance\n",
    "        1 - food\n",
    "        2 - noise\n",
    "        3 - service\n",
    "        None - all of the above (for control)\n",
    "    \"\"\"\n",
    "    # zero gradients\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # compute output\n",
    "    outputs = model(\n",
    "        **x,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "    first_layer = outputs.hidden_states[0]\n",
    "    last_layer = outputs.hidden_states[explanator.interchange_hidden_layer]\n",
    "\n",
    "    # get aspect representation\n",
    "    if aspect is not None:\n",
    "        start_idx = aspect*explanator.intervention_h_dim\n",
    "        end_idx = (aspect+1)*explanator.intervention_h_dim\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = outputs.hidden_states[-1].size(2)\n",
    "    \n",
    "    # compute backwards gradient from output logits to final layer\n",
    "    last_layer.retain_grad()\n",
    "\n",
    "    logits = outputs.logits[0] if isinstance(outputs.logits, tuple) else outputs.logits\n",
    "    classification = logits.argmax()\n",
    "    output_max = logits[0, classification]\n",
    "    output_max.backward(retain_graph=True)\n",
    "\n",
    "    last_layer_gradient = last_layer.grad.data.clone().detach()\n",
    "\n",
    "    # reset gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # compute backwards gradient from final layer to input layer,\n",
    "    # guided by the gradient from the logits to the final layer\n",
    "    first_layer.retain_grad()\n",
    "\n",
    "    \n",
    "    mask = torch.zeros_like(last_layer)\n",
    "    mask[:, 0, start_idx:end_idx] = last_layer_gradient[:, 0, start_idx:end_idx]\n",
    "    last_layer.backward(gradient=mask)\n",
    "\n",
    "    first_layer_gradient = first_layer.grad.data.clone().detach().cpu().numpy()\n",
    "\n",
    "    return first_layer_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Function\n",
    "\n",
    "This function uses one of the defined gradient-computation functions to visualize the aspect-specific saliency across an input sentence.\n",
    "This function allows the user to specify whether to show the control blackbox model, whether to normalize the output, and whether to select the maximum gradient to display by applying a key (such as `np.abs`), as well as other display flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_interventions_with_gradients(\n",
    "        sentence, \n",
    "        explanator, \n",
    "        aspect, \n",
    "        show_control=True,\n",
    "        normalize=True, \n",
    "        softmax_norm=False, \n",
    "        control_for_gradient=False, \n",
    "        saliency_fn=compute_guided_saliency,\n",
    "        grad_apply_fn=lambda x: x,\n",
    "        max_key_fn=lambda x: x,\n",
    "        display_fn=display_bar_plot\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Provides gradient-based saliency map to display which tokens \n",
    "    had the most effect on the intermediate representations\n",
    "    encoded by the `explanator.cpm_model` in the final layer.\n",
    "\n",
    "    sentence : str\n",
    "        Single input example\n",
    "    explanator : StaticCausalProxyModelForBERT \n",
    "    aspect : int\n",
    "        Chooses which embedded aspect to focus on.\n",
    "        0 - ambiance\n",
    "        1 - food\n",
    "        2 - noise\n",
    "        3 - service\n",
    "    show_control : bool, default True\n",
    "        Whether to display a second saliency plot, with \n",
    "    normalize : bool, default True\n",
    "        Whether to normalize output gradients\n",
    "    softmax_norm : bool, default False\n",
    "        Whether to apply softmax function in normalization (by default,\n",
    "        divide the gradients by their sum)\n",
    "    control_for_gradient : bool, default False\n",
    "        Whether to control for the change in gradients by computing \n",
    "        saliency both for the particular aspect representation and for \n",
    "        the whole final layer (our control), and plotting the difference\n",
    "        between these\n",
    "    saliency_fn\n",
    "        The function to use in order to compute gradient-based saliency\n",
    "        (right now, either `compute_saliency_of_final_layer`, `compute_weighted_saliency`, `compute_guided_saliency` or `compute_saliency_of_logits`)\n",
    "        NOTE: for `compute_saliency_of_logits`, set `show_control` to False, since the blackbox model does not have aspect-specific logits.\n",
    "    \"\"\"\n",
    "    explanator.blackbox_model.eval()\n",
    "    explanator.cpm_model.model.eval()\n",
    "\n",
    "    models = {'CPM': explanator.cpm_model.model}\n",
    "    if show_control:\n",
    "        models['control'] = explanator.blackbox_model\n",
    "\n",
    "    x = tokenize_sentence(sentence, explanator)\n",
    "    tokens = explanator.tokenizer.convert_ids_to_tokens(x['input_ids'][0])\n",
    "\n",
    "    plot_dfs = []\n",
    "    for model_name in models:\n",
    "        gradients = saliency_fn(models[model_name], x, explanator, aspect=aspect)\n",
    "        gradients = grad_apply_fn(gradients)\n",
    "        saliency = get_max(gradients, key_fn=max_key_fn)[0]\n",
    "\n",
    "        if control_for_gradient:\n",
    "            gradient_control = saliency_fn(models[model_name], x, explanator, aspect=None)\n",
    "            gradient_control = grad_apply_fn(gradient_control)\n",
    "            gradient_control = get_max(gradient_control, key_fn=max_key_fn)[0]\n",
    "            saliency = np.abs(saliency - gradient_control)\n",
    "\n",
    "        plot_df = pd.DataFrame({\n",
    "            'word': tokens,\n",
    "            'weight': saliency,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "        plot_dfs.append(plot_df)\n",
    "\n",
    "    if normalize:\n",
    "        for plot_df in plot_dfs:\n",
    "            if softmax_norm:\n",
    "                plot_df['weight'] = softmax(plot_df['weight'])\n",
    "            else:\n",
    "                plot_df['weight'] = plot_df['weight'] / plot_df['weight'].sum()\n",
    "\n",
    "    plot_data = pd.concat(plot_dfs)\n",
    "    \n",
    "    display_fn(plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Visualizations\n",
    "\n",
    "Graphs visualizations of which parts of the input the CPM model focuses on in representing a certain concept, such as ambiance, food, noise, or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'The music was too loud, and the decorations were tasteless, but they had friendly waiters and delicious pasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of ambiance?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, explainer, \n",
    "    0, normalize=True, softmax_norm=False, \n",
    "    control_for_gradient=False, \n",
    "    saliency_fn=compute_guided_saliency, grad_apply_fn=np.abs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of food?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, explainer, 1, \n",
    "    normalize=True, softmax_norm=False, \n",
    "    control_for_gradient=False, \n",
    "    saliency_fn=compute_guided_saliency, grad_apply_fn=np.abs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of noise?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, explainer, \n",
    "    2, normalize=True, softmax_norm=False, \n",
    "    control_for_gradient=False, \n",
    "    saliency_fn=compute_guided_saliency, grad_apply_fn=np.abs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of service?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, explainer, \n",
    "    3, normalize=True, softmax_norm=False, \n",
    "    control_for_gradient=False, \n",
    "    saliency_fn=compute_guided_saliency, grad_apply_fn=np.abs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of noise?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, cpm_explainer, 2, normalize=True, softmax_norm=False, control_for_gradient=False, saliency_fn=compute_guided_saliency, max_key_fn=np.abs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of noise?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, cpm_explainer, 2, normalize=True, softmax_norm=False, control_for_gradient=False, saliency_fn=compute_guided_saliency, max_key_fn=np.abs, display_fn=display_colored_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tokens influence the representation of service?\n",
    "explain_interventions_with_gradients(\n",
    "    example_sentence, cpm_explainer, 3, normalize=True, softmax_norm=False, control_for_gradient=False, saliency_fn=compute_guided_saliency, grad_apply_fn=np.abs, display_fn=display_colored_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "example_sentence = 'The music was too loud, and the decorations were tasteless, but they had friendly waiters and delicious pasta'\n",
    "aspects = ['ambiance', 'food', 'noise', 'service']\n",
    "\n",
    "for i, a in enumerate(aspects):\n",
    "    print(a)\n",
    "    explain_interventions_with_gradients(\n",
    "        example_sentence, \n",
    "        cpm_explainer, \n",
    "        i, \n",
    "        show_control=True,\n",
    "        normalize=True, \n",
    "        softmax_norm=False, \n",
    "        control_for_gradient=False, \n",
    "        saliency_fn=compute_guided_saliency,\n",
    "        grad_apply_fn=np.abs,\n",
    "        max_key_fn=lambda x: x,\n",
    "        display_fn=display_colored_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'The music was too loud, and the decorations were tasteless, but they had friendly waiters and delicious pasta'\n",
    "aspects = ['ambiance', 'food', 'noise', 'service']\n",
    "\n",
    "for i, a in enumerate(aspects):\n",
    "    print(a)\n",
    "    explain_interventions_with_gradients(\n",
    "        example_sentence, \n",
    "        cpm_explainer, \n",
    "        i, \n",
    "        show_control=True,\n",
    "        normalize=True, \n",
    "        softmax_norm=False, \n",
    "        control_for_gradient=False, \n",
    "        saliency_fn=compute_guided_saliency,\n",
    "        grad_apply_fn=lambda x: x,\n",
    "        max_key_fn=np.abs,\n",
    "        display_fn=display_colored_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'The music was soothing, and the decorations were pleasant. I especially liked the friendly waiters and delicious pasta'\n",
    "aspects = ['ambiance', 'food', 'noise', 'service']\n",
    "\n",
    "for i, a in enumerate(aspects):\n",
    "    print(a)\n",
    "    explain_interventions_with_gradients(\n",
    "        example_sentence, \n",
    "        cpm_explainer, \n",
    "        i, \n",
    "        show_control=True,\n",
    "        normalize=False, \n",
    "        softmax_norm=False, \n",
    "        control_for_gradient=False, \n",
    "        saliency_fn=compute_guided_saliency,\n",
    "        grad_apply_fn=lambda x: x,\n",
    "        max_key_fn=np.abs,\n",
    "        display_fn=display_colored_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'The music was bothersome, and there were no decorations whatsoever. Not to mention, the waiters were rude and the pasta was cold'\n",
    "aspects = ['ambiance', 'food', 'noise', 'service']\n",
    "\n",
    "for i, a in enumerate(aspects):\n",
    "    print(a)\n",
    "    explain_interventions_with_gradients(\n",
    "        example_sentence, \n",
    "        cpm_explainer, \n",
    "        i, \n",
    "        show_control=True,\n",
    "        normalize=True, \n",
    "        softmax_norm=False, \n",
    "        control_for_gradient=False, \n",
    "        saliency_fn=compute_guided_saliency,\n",
    "        grad_apply_fn=lambda x: x,\n",
    "        max_key_fn=np.abs,\n",
    "        display_fn=display_colored_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n",
    "\n",
    "Running gradient-based visualizations for the CPM model, using `captum.ai`'s implementation of integrated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly modifying `LayerIntegratedGradients` to give us the opportunity of running gradients through the hidden layer of the CPM.\n",
    "This requires modifying the `_run_forward()` function so that it can return the hidden layers, and modifying the `gradient_func` used in the integrated gradients computation so that it computes the gradient through the aspect representations that we specify. \n",
    "Note that, at least with this implementation, computing the convergence delta won't work (but I don't think it should take too many changes to fix this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import functools\n",
    "import warnings\n",
    "from typing import Any, Callable, List, overload, Tuple, Union\n",
    "\n",
    "from captum._utils.common import (\n",
    "    _extract_device,\n",
    "    _format_additional_forward_args,\n",
    "    _format_outputs,\n",
    ")\n",
    "from captum._utils.gradient import _forward_layer_eval #, _run_forward\n",
    "#################### IMPORT MODULES ###########################\n",
    "from inspect import signature\n",
    "from captum._utils.common import _select_targets, _format_input\n",
    "###############################################################\n",
    "from captum._utils.typing import BaselineType, Literal, ModuleOrModuleList, TargetType\n",
    "from captum.attr._core.integrated_gradients import IntegratedGradients\n",
    "from captum.attr._utils.attribution import GradientAttribution, LayerAttribution\n",
    "from captum.attr._utils.common import (\n",
    "    _format_input_baseline,\n",
    "    _tensorize_baseline,\n",
    "    _validate_input,\n",
    ")\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor\n",
    "from torch.nn.parallel.scatter_gather import scatter\n",
    "\n",
    "############### A FEW CONSTANTS (SHOULD LATER INTEGRATE AS ARGUMENTS) ####################\n",
    "INTERVENTION_H_DIM = 192\n",
    "MODEL_H_DIM = 768\n",
    "INTERCHANGE_LAYER = interchange_layer\n",
    "CONTROL = False\n",
    "\n",
    "####################### OVERRIDE RUN FORWARD FUNCTION ####################################\n",
    "def _run_forward(\n",
    "    forward_func: Callable,\n",
    "    inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "    target: TargetType = None,\n",
    "    additional_forward_args: Any = None,\n",
    ") -> Tensor:\n",
    "    forward_func_args = signature(forward_func).parameters\n",
    "    if len(forward_func_args) == 0:\n",
    "        output = forward_func()\n",
    "        return output if target is None else _select_targets(output, target)\n",
    "\n",
    "    # make everything a tuple so that it is easy to unpack without\n",
    "    # using if-statements\n",
    "    inputs = _format_input(inputs)\n",
    "    additional_forward_args = _format_additional_forward_args(additional_forward_args)\n",
    "\n",
    "    output = forward_func(\n",
    "        *(*inputs, *additional_forward_args)\n",
    "        if additional_forward_args is not None\n",
    "        else inputs,\n",
    "    )\n",
    "    if CONTROL:\n",
    "        logits = output.logits # take first logit for overall sentiment\n",
    "    else:\n",
    "        logits = output.logits[0] # take first logit for overall sentiment\n",
    "    final_activation = output.hidden_states[INTERCHANGE_LAYER]\n",
    "    return _select_targets(logits, target), final_activation\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "class LayerIntegratedGradients(LayerAttribution, GradientAttribution):\n",
    "    r\"\"\"\n",
    "    Layer Integrated Gradients is a variant of Integrated Gradients that assigns\n",
    "    an importance score to layer inputs or outputs, depending on whether we\n",
    "    attribute to the former or to the latter one.\n",
    "\n",
    "    Integrated Gradients is an axiomatic model interpretability algorithm that\n",
    "    attributes / assigns an importance score to each input feature by approximating\n",
    "    the integral of gradients of the model's output with respect to the inputs\n",
    "    along the path (straight line) from given baselines / references to inputs.\n",
    "\n",
    "    Baselines can be provided as input arguments to attribute method.\n",
    "    To approximate the integral we can choose to use either a variant of\n",
    "    Riemann sum or Gauss-Legendre quadrature rule.\n",
    "\n",
    "    More details regarding the integrated gradients method can be found in the\n",
    "    original paper:\n",
    "    https://arxiv.org/abs/1703.01365\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable,\n",
    "        layer: ModuleOrModuleList,\n",
    "        device_ids: Union[None, List[int]] = None,\n",
    "        multiply_by_inputs: bool = True,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            forward_func (callable):  The forward function of the model or any\n",
    "                        modification of it\n",
    "            layer (ModuleOrModuleList):\n",
    "                        Layer or list of layers for which attributions are computed.\n",
    "                        For each layer the output size of the attribute matches\n",
    "                        this layer's input or output dimensions, depending on\n",
    "                        whether we attribute to the inputs or outputs of the\n",
    "                        layer, corresponding to the attribution of each neuron\n",
    "                        in the input or output of this layer.\n",
    "\n",
    "                        Please note that layers to attribute on cannot be\n",
    "                        dependent on each other. That is, a subset of layers in\n",
    "                        `layer` cannot produce the inputs for another layer.\n",
    "\n",
    "                        For example, if your model is of a simple linked-list\n",
    "                        based graph structure (think nn.Sequence), e.g. x -> l1\n",
    "                        -> l2 -> l3 -> output. If you pass in any one of those\n",
    "                        layers, you cannot pass in another due to the\n",
    "                        dependence, e.g.  if you pass in l2 you cannot pass in\n",
    "                        l1 or l3.\n",
    "\n",
    "            device_ids (list(int)): Device ID list, necessary only if forward_func\n",
    "                        applies a DataParallel model. This allows reconstruction of\n",
    "                        intermediate outputs from batched results across devices.\n",
    "                        If forward_func is given as the DataParallel model itself,\n",
    "                        then it is not necessary to provide this argument.\n",
    "            multiply_by_inputs (bool, optional): Indicates whether to factor\n",
    "                        model inputs' multiplier in the final attribution scores.\n",
    "                        In the literature this is also known as local vs global\n",
    "                        attribution. If inputs' multiplier isn't factored in,\n",
    "                        then this type of attribution method is also called local\n",
    "                        attribution. If it is, then that type of attribution\n",
    "                        method is called global.\n",
    "                        More detailed can be found here:\n",
    "                        https://arxiv.org/abs/1711.06104\n",
    "\n",
    "                        In case of layer integrated gradients, if `multiply_by_inputs`\n",
    "                        is set to True, final sensitivity scores are being multiplied by\n",
    "                        layer activations for inputs - layer activations for baselines.\n",
    "\n",
    "        \"\"\"\n",
    "        LayerAttribution.__init__(self, forward_func, layer, device_ids=device_ids)\n",
    "        GradientAttribution.__init__(self, forward_func)\n",
    "        self.ig = IntegratedGradients(forward_func, multiply_by_inputs)\n",
    "\n",
    "        if isinstance(layer, list) and len(layer) > 1:\n",
    "            warnings.warn(\n",
    "                \"Multiple layers provided. Please ensure that each layer is\"\n",
    "                \"**not** solely solely dependent on the outputs of\"\n",
    "                \"another layer. Please refer to the documentation for more\"\n",
    "                \"detail.\"\n",
    "            )\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType,\n",
    "        target: TargetType,\n",
    "        additional_forward_args: Any,\n",
    "        n_steps: int,\n",
    "        method: str,\n",
    "        internal_batch_size: Union[None, int],\n",
    "        return_convergence_delta: Literal[False],\n",
    "        attribute_to_layer_input: bool,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType,\n",
    "        target: TargetType,\n",
    "        additional_forward_args: Any,\n",
    "        n_steps: int,\n",
    "        method: str,\n",
    "        internal_batch_size: Union[None, int],\n",
    "        return_convergence_delta: Literal[True],\n",
    "        attribute_to_layer_input: bool,\n",
    "    ) -> Tuple[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tensor,\n",
    "    ]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Any = None,\n",
    "        n_steps: int = 50,\n",
    "        method: str = \"gausslegendre\",\n",
    "        internal_batch_size: Union[None, int] = None,\n",
    "        return_convergence_delta: bool = False,\n",
    "        attribute_to_layer_input: bool = False,\n",
    "    ) -> Union[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tuple[\n",
    "            Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "            Tensor,\n",
    "        ],\n",
    "    ]:\n",
    "        ...\n",
    "\n",
    "    @log_usage()\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Any = None,\n",
    "        n_steps: int = 50,\n",
    "        method: str = \"gausslegendre\",\n",
    "        internal_batch_size: Union[None, int] = None,\n",
    "        return_convergence_delta: bool = False,\n",
    "        attribute_to_layer_input: bool = False,\n",
    "        ############## NEW ARGUMENT: ASPECT OF INTEREST ############################\n",
    "        aspect: Union[int, None] = None\n",
    "        ############################################################################\n",
    "    ) -> Union[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tuple[\n",
    "            Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "            Tensor,\n",
    "        ],\n",
    "    ]:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to layer inputs or outputs of the model, depending on whether\n",
    "        `attribute_to_layer_input` is set to True or False, using the approach\n",
    "        described above.\n",
    "\n",
    "        In addition to that it also returns, if `return_convergence_delta` is\n",
    "        set to True, integral approximation delta based on the completeness\n",
    "        property of integrated gradients.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (tensor or tuple of tensors):  Input for which layer integrated\n",
    "                        gradients are computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, tensor, tuple of scalars or tensors, optional):\n",
    "                        Baselines define the starting point from which integral\n",
    "                        is computed and can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "                            - either a tensor with matching dimensions to\n",
    "                              corresponding tensor in the inputs' tuple\n",
    "                              or the first dimension is one and the remaining\n",
    "                              dimensions match with the corresponding\n",
    "                              input tensor.\n",
    "                            - or a scalar, corresponding to a tensor in the\n",
    "                              inputs' tuple. This scalar value is broadcasted\n",
    "                              for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` is not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "\n",
    "                        Default: None\n",
    "            target (int, tuple, tensor or list, optional):  Output indices for\n",
    "                        which gradients are computed (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            n_steps (int, optional): The number of steps used by the approximation\n",
    "                        method. Default: 50.\n",
    "            method (string, optional): Method for approximating the integral,\n",
    "                        one of `riemann_right`, `riemann_left`, `riemann_middle`,\n",
    "                        `riemann_trapezoid` or `gausslegendre`.\n",
    "                        Default: `gausslegendre` if no method is provided.\n",
    "            internal_batch_size (int, optional): Divides total #steps * #examples\n",
    "                        data points into chunks of size at most internal_batch_size,\n",
    "                        which are computed (forward / backward passes)\n",
    "                        sequentially. internal_batch_size must be at least equal to\n",
    "                        #examples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain internal_batch_size / num_devices examples.\n",
    "                        If internal_batch_size is None, then all evaluations are\n",
    "                        processed in one batch.\n",
    "                        Default: None\n",
    "            return_convergence_delta (bool, optional): Indicates whether to return\n",
    "                        convergence delta or not. If `return_convergence_delta`\n",
    "                        is set to True convergence delta will be returned in\n",
    "                        a tuple following attributions.\n",
    "                        Default: False\n",
    "            attribute_to_layer_input (bool, optional): Indicates whether to\n",
    "                        compute the attribution with respect to the layer input\n",
    "                        or output. If `attribute_to_layer_input` is set to True\n",
    "                        then the attributions will be computed with respect to\n",
    "                        layer input, otherwise it will be computed with respect\n",
    "                        to layer output.\n",
    "                        Note that currently it is assumed that either the input\n",
    "                        or the output of internal layer, depending on whether we\n",
    "                        attribute to the input or output, is a single tensor.\n",
    "                        Support for multiple tensors will be added later.\n",
    "                        Default: False\n",
    "            Returns:\n",
    "                **attributions** or 2-element tuple of **attributions**, **delta**:\n",
    "                - **attributions** (*tensor*, tuple of *tensors* or tuple of *tensors*):\n",
    "                        Integrated gradients with respect to `layer`'s inputs or\n",
    "                        outputs. Attributions will always be the same size and\n",
    "                        dimensionality as the input or output of the given layer,\n",
    "                        depending on whether we attribute to the inputs or outputs\n",
    "                        of the layer which is decided by the input flag\n",
    "                        `attribute_to_layer_input`.\n",
    "\n",
    "                        For a single layer, attributions are returned in a tuple if\n",
    "                        the layer inputs / outputs contain multiple tensors,\n",
    "                        otherwise a single tensor is returned.\n",
    "\n",
    "                        For multiple layers, attributions will always be\n",
    "                        returned as a list. Each element in this list will be\n",
    "                        equivalent to that of a single layer output, i.e. in the\n",
    "                        case that one layer, in the given layers, inputs / outputs\n",
    "                        multiple tensors: the corresponding output element will be\n",
    "                        a tuple of tensors. The ordering of the outputs will be\n",
    "                        the same order as the layers given in the constructor.\n",
    "                - **delta** (*tensor*, returned if return_convergence_delta=True):\n",
    "                        The difference between the total approximated and true\n",
    "                        integrated gradients. This is computed using the property\n",
    "                        that the total sum of forward_func(inputs) -\n",
    "                        forward_func(baselines) must equal the total sum of the\n",
    "                        integrated gradient.\n",
    "                        Delta is calculated per example, meaning that the number of\n",
    "                        elements in returned delta tensor is equal to the number of\n",
    "                        of examples in inputs.\n",
    "\n",
    "            Examples::\n",
    "\n",
    "                >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,\n",
    "                >>> # and returns an Nx10 tensor of class probabilities.\n",
    "                >>> # It contains an attribute conv1, which is an instance of nn.conv2d,\n",
    "                >>> # and the output of this layer has dimensions Nx12x32x32.\n",
    "                >>> net = ImageClassifier()\n",
    "                >>> lig = LayerIntegratedGradients(net, net.conv1)\n",
    "                >>> input = torch.randn(2, 3, 32, 32, requires_grad=True)\n",
    "                >>> # Computes layer integrated gradients for class 3.\n",
    "                >>> # attribution size matches layer output, Nx12x32x32\n",
    "                >>> attribution = lig.attribute(input, target=3)\n",
    "        \"\"\"\n",
    "        inps, baselines = _format_input_baseline(inputs, baselines)\n",
    "        _validate_input(inps, baselines, n_steps, method)\n",
    "\n",
    "        baselines = _tensorize_baseline(inps, baselines)\n",
    "        additional_forward_args = _format_additional_forward_args(\n",
    "            additional_forward_args\n",
    "        )\n",
    "\n",
    "        def flatten_tuple(tup):\n",
    "            return tuple(\n",
    "                sum((list(x) if isinstance(x, (tuple, list)) else [x] for x in tup), [])\n",
    "            )\n",
    "\n",
    "        if self.device_ids is None:\n",
    "            self.device_ids = getattr(self.forward_func, \"device_ids\", None)\n",
    "\n",
    "        inputs_layer = _forward_layer_eval(\n",
    "            self.forward_func,\n",
    "            inps,\n",
    "            self.layer,\n",
    "            device_ids=self.device_ids,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            attribute_to_layer_input=attribute_to_layer_input,\n",
    "        )\n",
    "\n",
    "        # if we have one output\n",
    "        if not isinstance(self.layer, list):\n",
    "            inputs_layer = (inputs_layer,)\n",
    "\n",
    "        num_outputs = [1 if isinstance(x, Tensor) else len(x) for x in inputs_layer]\n",
    "        num_outputs_cumsum = torch.cumsum(\n",
    "            torch.IntTensor([0] + num_outputs), dim=0  # type: ignore\n",
    "        )\n",
    "        inputs_layer = flatten_tuple(inputs_layer)\n",
    "\n",
    "        baselines_layer = _forward_layer_eval(\n",
    "            self.forward_func,\n",
    "            baselines,\n",
    "            self.layer,\n",
    "            device_ids=self.device_ids,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            attribute_to_layer_input=attribute_to_layer_input,\n",
    "        )\n",
    "        baselines_layer = flatten_tuple(baselines_layer)\n",
    "\n",
    "        # inputs -> these inputs are scaled\n",
    "        def gradient_func(\n",
    "            forward_fn: Callable,\n",
    "            inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "            target_ind: TargetType = None,\n",
    "            additional_forward_args: Any = None,\n",
    "        ) -> Tuple[Tensor, ...]:\n",
    "            if self.device_ids is None or len(self.device_ids) == 0:\n",
    "                scattered_inputs = (inputs,)\n",
    "            else:\n",
    "                # scatter method does not have a precise enough return type in its\n",
    "                # stub, so suppress the type warning.\n",
    "                scattered_inputs = scatter(  # type:ignore\n",
    "                    inputs, target_gpus=self.device_ids\n",
    "                )\n",
    "\n",
    "            scattered_inputs_dict = {\n",
    "                scattered_input[0].device: scattered_input\n",
    "                for scattered_input in scattered_inputs\n",
    "            }\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(True):\n",
    "\n",
    "                def layer_forward_hook(\n",
    "                    module, hook_inputs, hook_outputs=None, layer_idx=0\n",
    "                ):\n",
    "                    device = _extract_device(module, hook_inputs, hook_outputs)\n",
    "                    is_layer_tuple = (\n",
    "                        isinstance(hook_outputs, tuple)\n",
    "                        # hook_outputs is None if attribute_to_layer_input == True\n",
    "                        if hook_outputs is not None\n",
    "                        else isinstance(hook_inputs, tuple)\n",
    "                    )\n",
    "\n",
    "                    if is_layer_tuple:\n",
    "                        return scattered_inputs_dict[device][\n",
    "                            num_outputs_cumsum[layer_idx] : num_outputs_cumsum[\n",
    "                                layer_idx + 1\n",
    "                            ]\n",
    "                        ]\n",
    "\n",
    "                    return scattered_inputs_dict[device][num_outputs_cumsum[layer_idx]]\n",
    "\n",
    "                hooks = []\n",
    "                try:\n",
    "\n",
    "                    layers = self.layer\n",
    "                    if not isinstance(layers, list):\n",
    "                        layers = [self.layer]\n",
    "\n",
    "                    for layer_idx, layer in enumerate(layers):\n",
    "                        hook = None\n",
    "                        # TODO:\n",
    "                        # Allow multiple attribute_to_layer_input flags for\n",
    "                        # each layer, i.e. attribute_to_layer_input[layer_idx]\n",
    "                        if attribute_to_layer_input:\n",
    "                            hook = layer.register_forward_pre_hook(\n",
    "                                functools.partial(\n",
    "                                    layer_forward_hook, layer_idx=layer_idx\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            hook = layer.register_forward_hook(\n",
    "                                functools.partial(\n",
    "                                    layer_forward_hook, layer_idx=layer_idx\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                        hooks.append(hook)\n",
    "\n",
    "                    output = _run_forward(\n",
    "                        self.forward_func, tuple(), target_ind, additional_forward_args\n",
    "                    )\n",
    "                finally:\n",
    "                    for hook in hooks:\n",
    "                        if hook is not None:\n",
    "                            hook.remove()\n",
    "\n",
    "                # assert output[0].numel() == 1, (\n",
    "                #     \"Target not provided when necessary, cannot\"\n",
    "                #     \" take gradient with respect to multiple outputs.\"\n",
    "                # )\n",
    "                # torch.unbind(forward_out) is a list of scalar tensor tuples and\n",
    "                # contains batch_size * #steps elements\n",
    "                # grads = torch.autograd.grad(torch.unbind(output), inputs)\n",
    "\n",
    "                ##################### TAKE GRADIENT THROUGH ACTIVATION #########################\n",
    "                model_output, final_layer_activation = output\n",
    "\n",
    "                # get aspect representation\n",
    "                if aspect is not None:\n",
    "                    start_idx = aspect * INTERVENTION_H_DIM\n",
    "                    end_idx = (aspect+1) * INTERVENTION_H_DIM\n",
    "                else:\n",
    "                    start_idx = 0\n",
    "                    # end_idx = model_output.hidden_states[-1].size(2)\n",
    "                    end_idx = MODEL_H_DIM\n",
    "\n",
    "                # compute backwards gradient from output logits to final layer\n",
    "                final_layer_gradient = torch.autograd.grad(\n",
    "                    outputs=torch.unbind(model_output),\n",
    "                    inputs=final_layer_activation,\n",
    "                    retain_graph=True\n",
    "                )\n",
    "\n",
    "                final_layer_gradient = torch.stack(final_layer_gradient).squeeze(0).clone().detach()\n",
    "\n",
    "                # reset gradients ??\n",
    "\n",
    "                # compute backwards gradient from final layer to input layer,\n",
    "                # guided by the gradient from the logits to the final layer\n",
    "                mask = torch.zeros_like(final_layer_activation)\n",
    "                mask[:, 0, start_idx:end_idx] = final_layer_gradient[:, 0, start_idx:end_idx]\n",
    "\n",
    "                grads = torch.autograd.grad(\n",
    "                    outputs=torch.unbind(final_layer_activation),\n",
    "                    inputs=inputs,\n",
    "                    grad_outputs=torch.unbind(mask)\n",
    "                )\n",
    "                ###############################################################################\n",
    "\n",
    "            return grads\n",
    "\n",
    "        self.ig.gradient_func = gradient_func\n",
    "        all_inputs = (\n",
    "            (inps + additional_forward_args)\n",
    "            if additional_forward_args is not None\n",
    "            else inps\n",
    "        )\n",
    "\n",
    "        attributions = self.ig.attribute.__wrapped__(  # type: ignore\n",
    "            self.ig,  # self\n",
    "            inputs_layer,\n",
    "            baselines=baselines_layer,\n",
    "            target=target,\n",
    "            additional_forward_args=all_inputs,\n",
    "            n_steps=n_steps,\n",
    "            method=method,\n",
    "            internal_batch_size=internal_batch_size,\n",
    "            return_convergence_delta=False,\n",
    "        )\n",
    "\n",
    "        # handle multiple outputs\n",
    "        output: List[Tuple[Tensor, ...]] = [\n",
    "            tuple(\n",
    "                attributions[\n",
    "                    int(num_outputs_cumsum[i]) : int(num_outputs_cumsum[i + 1])\n",
    "                ]\n",
    "            )\n",
    "            for i in range(len(num_outputs))\n",
    "        ]\n",
    "\n",
    "        if return_convergence_delta:\n",
    "            start_point, end_point = baselines, inps\n",
    "            # computes approximation error based on the completeness axiom\n",
    "            delta = self.compute_convergence_delta(\n",
    "                attributions,\n",
    "                start_point,\n",
    "                end_point,\n",
    "                additional_forward_args=additional_forward_args,\n",
    "                target=target,\n",
    "            )\n",
    "            return _format_outputs(isinstance(self.layer, list), output), delta\n",
    "        return _format_outputs(isinstance(self.layer, list), output)\n",
    "\n",
    "\n",
    "    def has_convergence_delta(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def multiplies_by_inputs(self):\n",
    "        return self.ig.multiplies_by_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from CS 224u that uses `LayerIntegratedGradients` to compute integrated gradients for a BERT model.\n",
    "Note that we will need to modify this code slightly for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_predict_one_proba(text, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, add_special_tokens=True, return_tensors='pt').to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "        # case when cpm returns multiple predictions, take the first\n",
    "        # (corresponding to overall sentiment)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits[0]\n",
    "        preds = F.softmax(logits, dim=1)\n",
    "    model.train()\n",
    "    return preds.squeeze(0)\n",
    "\n",
    "\n",
    "def hf_ig_encodings(text, tokenizer):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    base_ids = [pad_id] * len(input_ids)\n",
    "    input_ids = [cls_id] + input_ids + [sep_id]\n",
    "    base_ids = [cls_id] + base_ids + [sep_id]\n",
    "    return torch.LongTensor([input_ids]), torch.LongTensor([base_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECTS = ['ambiance', 'food', 'noise', 'service']\n",
    "asp_to_ind = {\n",
    "    'overall': None,\n",
    "    **({a: i for i, a in enumerate(ASPECTS)})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_ig_analyses(inputs, targets, tokenizer, model):\n",
    "    data = []\n",
    "    for text, true_class in zip(inputs, targets):\n",
    "        # create visualization for each aspect \n",
    "        for aspect, _ in enumerate(ASPECTS):\n",
    "            score_vis = hf_ig_analysis_one(text, true_class, tokenizer, model, aspect=aspect)\n",
    "            data.append(score_vis)\n",
    "    html = viz.visualize_text(data)\n",
    "\n",
    "def hf_ig_analyses_new(inputs, targets, tokenizer, model):\n",
    "    data = []\n",
    "    for text in inputs:\n",
    "        # create visualization for each aspect \n",
    "        for aspect, _ in enumerate(ASPECTS):\n",
    "            score_vis = hf_ig_analyses_multiple(text, targets, tokenizer, model, aspect=aspect)\n",
    "            data.append(score_vis)\n",
    "    html = viz.visualize_text(data)\n",
    "    \n",
    "def hf_ig_analyses_multiple(text, true_classes, tokenizer, model, device=0, aspect=None):\n",
    "    \n",
    "    scores_for_true_classes = []\n",
    "    for true_class in true_classes:\n",
    "        # Option to look at different layers:\n",
    "        # layer = model.roberta.encoder.layer[0]\n",
    "        # layer = model.roberta.embeddings.word_embeddings\n",
    "        layer = model.bert.embeddings\n",
    "\n",
    "        def ig_forward(inputs):\n",
    "            # logits at first index, cpm returns multiple logits\n",
    "            # logits = model(inputs).logits\n",
    "            # return logits[pred_ind] if isinstance(logits, tuple) else logits\n",
    "            return model(inputs, output_hidden_states=True)\n",
    "\n",
    "        ig = LayerIntegratedGradients(ig_forward, layer)\n",
    "\n",
    "        input_ids, base_ids = hf_ig_encodings(text, tokenizer)\n",
    "\n",
    "        attrs = ig.attribute(\n",
    "            input_ids.to(model.device),\n",
    "            base_ids.to(model.device),\n",
    "            target=true_class,\n",
    "            return_convergence_delta=False,\n",
    "            attribute_to_layer_input=False,\n",
    "            aspect=aspect\n",
    "        )\n",
    "\n",
    "        # Summarize and z-score normalize the attributions\n",
    "        # for each representation in `layer`:\n",
    "        scores = attrs.sum(dim=-1).squeeze(0)\n",
    "        scores_for_true_classes += [scores]\n",
    "    scores_for_true_classes = torch.stack(scores_for_true_classes, dim=0).sum(dim=0)\n",
    "    scores = scores_for_true_classes\n",
    "    \n",
    "    scores = (scores - scores.mean()) / scores.norm()\n",
    "    print(scores)\n",
    "    \n",
    "    # Intuitive tokens to help with analysis:\n",
    "    raw_input = tokenizer.convert_ids_to_tokens(input_ids.tolist()[0])\n",
    "    # RoBERTa-specific clean-up:\n",
    "    # raw_input = [x.strip(\"\") for x in raw_input]\n",
    "\n",
    "    # Predictions for comparisons:\n",
    "    pred_probs = hf_predict_one_proba(text, tokenizer, model)\n",
    "    pred_class = pred_probs.argmax()\n",
    "\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "        word_attributions=scores,\n",
    "        pred_prob=pred_probs.max(),\n",
    "        pred_class=pred_class,\n",
    "        true_class=true_class,\n",
    "        # override attribution label with aspect label, to make the visualization clearer\n",
    "        attr_class=ASPECTS[aspect] if aspect is not None else None,\n",
    "        attr_score=attrs.sum(),\n",
    "        raw_input_ids=raw_input,\n",
    "        convergence_score=None)\n",
    "    \n",
    "    max_score = max([abs(score) for score in scores.tolist()])\n",
    "    mag = 1.0/max_score\n",
    "    word_idx = 0\n",
    "    latex_str = \"\"\n",
    "    for score in scores.tolist():\n",
    "        sign = \"green\" if score > 0 else \"red\"\n",
    "        color_mag = int(abs(score*mag)*100)\n",
    "        word = raw_input[word_idx]\n",
    "        latex_str += \"\\\\colorbox{\"+sign+\"!\"+str(color_mag)+\"}{\\\\strut \"+word+\"}\"\n",
    "        word_idx += 1\n",
    "        \n",
    "    print(latex_str)\n",
    "    print()\n",
    "    \n",
    "    return score_vis\n",
    "    \n",
    "def hf_ig_analysis_one(text, true_class, tokenizer, model, device=0, aspect=None):\n",
    "    # Option to look at different layers:\n",
    "    # layer = model.roberta.encoder.layer[0]\n",
    "    # layer = model.roberta.embeddings.word_embeddings\n",
    "    layer = model.bert.embeddings\n",
    "\n",
    "    def ig_forward(inputs):\n",
    "        # logits at first index, cpm returns multiple logits\n",
    "        # logits = model(inputs).logits\n",
    "        # return logits[pred_ind] if isinstance(logits, tuple) else logits\n",
    "        return model(inputs, output_hidden_states=True)\n",
    "\n",
    "    ig = LayerIntegratedGradients(ig_forward, layer)\n",
    "\n",
    "    input_ids, base_ids = hf_ig_encodings(text, tokenizer)\n",
    "\n",
    "    attrs = ig.attribute(\n",
    "        input_ids.to(model.device),\n",
    "        base_ids.to(model.device),\n",
    "        target=true_class,\n",
    "        return_convergence_delta=False,\n",
    "        attribute_to_layer_input=False,\n",
    "        aspect=aspect\n",
    "    )\n",
    "\n",
    "    # Summarize and z-score normalize the attributions\n",
    "    # for each representation in `layer`:\n",
    "    scores = attrs.sum(dim=-1).squeeze(0)\n",
    "    scores = (scores - scores.mean()) / scores.norm()\n",
    "    \n",
    "    # Intuitive tokens to help with analysis:\n",
    "    raw_input = tokenizer.convert_ids_to_tokens(input_ids.tolist()[0])\n",
    "    # RoBERTa-specific clean-up:\n",
    "    # raw_input = [x.strip(\"\") for x in raw_input]\n",
    "\n",
    "    # Predictions for comparisons:\n",
    "    pred_probs = hf_predict_one_proba(text, tokenizer, model)\n",
    "    pred_class = pred_probs.argmax()\n",
    "\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "        word_attributions=scores,\n",
    "        pred_prob=pred_probs.max(),\n",
    "        pred_class=pred_class,\n",
    "        true_class=true_class,\n",
    "        # override attribution label with aspect label, to make the visualization clearer\n",
    "        attr_class=ASPECTS[aspect] if aspect is not None else None,\n",
    "        attr_score=attrs.sum(),\n",
    "        raw_input_ids=raw_input,\n",
    "        convergence_score=None)\n",
    "    \n",
    "    max_score = max([abs(score) for score in scores.tolist()])\n",
    "    mag = 1.0/max_score\n",
    "    word_idx = 0\n",
    "    latex_str = \"\"\n",
    "    for score in scores.tolist():\n",
    "        sign = \"green\" if score > 0 else \"red\"\n",
    "        color_mag = int(abs(score*mag)*100)\n",
    "        word = raw_input[word_idx]\n",
    "        latex_str += \"\\\\colorbox{\"+sign+\"!\"+str(color_mag)+\"}{\\\\strut \"+word+\"}\"\n",
    "        word_idx += 1\n",
    "        \n",
    "    print(latex_str)\n",
    "    print()\n",
    "    \n",
    "    return score_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create integrated gradients visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_sentences = [\n",
    "#     # all negative\n",
    "#     \"Horrible experience! Distasteful decorations, slow service, bland food, and all the while with music blasting from the walls\",\n",
    "#     # all neutral (I tried?)\n",
    "#     \"The decorations were okay, the servers were meh, the pasta so-so, and the music not my taste, but also not bad\",\n",
    "#     # all positive\n",
    "#     \"Wonderful place all around! Kind waiters, good vibes, and pleasant music - not to mention the best sushi I ever had\",\n",
    "#     # some negative, some positive\n",
    "#     \"Although the decorations were tasteless and the music was far too loud, the food arrived quickly, and the dessert was incredible\",\n",
    "#     # some negative, some unmentioned\n",
    "#     \"Tacky decorations, deafening music :(\",\n",
    "#     # some positive, some unmentioned\n",
    "#     \"Kind waiters, awesome fish tacos!\"\n",
    "# ]\n",
    "\n",
    "# labels = [0, 2, 4, 3, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    'The music was too loud, and the decorations were tasteless, but they had friendly waiters and delicious pasta'\n",
    "]\n",
    "\n",
    "labels = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL = False\n",
    "_ = random.seed(seed)\n",
    "_ = np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "hf_ig_analyses_new(\n",
    "    example_sentences, labels, explainer.tokenizer, \n",
    "    explainer.cpm_model.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL = True\n",
    "_ = random.seed(seed)\n",
    "_ = np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "hf_ig_analyses(\n",
    "    example_sentences, labels, explainer.tokenizer, \n",
    "    explainer.blackbox_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4eadbeb4763107051c423ffe9ee9ffcb0adb376f6d3c57fb486fb39aa2d1a822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
