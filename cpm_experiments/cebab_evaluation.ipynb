{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating CPM with CEBaB\n",
    "Our Causal Proxy Model (CPM) is for providing concept-based explanation for a blackbox model. We use newly developed CEBaB benchmark for comparing CPM with other concept-based explanation methods. This notebook evaluates CPM with CEBaB benchmark under different settings.\n",
    "\n",
    "More importantly, we introduce new baselines for CPM as well. Formally, we evaluate the blackbox model with interchange intervention evaluation (which will be introduced in details below).\n",
    "\n",
    "In this notebook, we can evaluate the following models:\n",
    "- CPM: `BERT-base-uncased`\n",
    "- CPM: `RoBERTa-base`\n",
    "- CPM: `GPT2`\n",
    "- CPM: `LSTM+GloVe`\n",
    "- CPM: `Control`\n",
    "\n",
    "and we can evaluate with the following conditions:\n",
    "- 2-class\n",
    "- 3-class\n",
    "- 5-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *\n",
    "\"\"\"\n",
    "For evaluate, we use a single random seed, as\n",
    "the models are trained with 5 different seeds\n",
    "already.\n",
    "\"\"\"\n",
    "_ = random.seed(123)\n",
    "_ = np.random.seed(123)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main evaluate script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following blocks will run CEBaB benchmark in\n",
    "all the combinations of the following conditions.\n",
    "\"\"\"\n",
    "grid = {\n",
    "    \"seed\": [42, 66, 77, 88, 99],\n",
    "    \"h_dim\": [192],\n",
    "    \"class_num\": [2, 3, 5],\n",
    "    \"control\": [True],\n",
    "    \"beta\" : [1.0],\n",
    "    \"gemma\" : [3.0],\n",
    "    \"cls_dropout\" : [0.1],\n",
    "    \"enc_dropout\" : [0.1],\n",
    "    \"model_arch\" : [\"bert-base-uncased\"]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "device = 'cuda:8'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 42), ('class_num', 2), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '2-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63bc12ea44241479dca1cc7efa13c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eval_pipeline/utils/data_utils.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['review_majority'] = dataset['review_majority'].apply(lambda score: encoding[score])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "Dropped 391 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 72/72 [00:14<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 42), ('class_num', 3), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '3-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d07daa69cd4b17a2b539a14e3c5fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.3-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 42), ('class_num', 5), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '5-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472dcb40f39448f69cee208ad7a0ee1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 66), ('class_num', 2), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '2-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ef74cadabb4623b9392f148183247f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "Dropped 391 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_66 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 72/72 [00:14<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 66), ('class_num', 3), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '3-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c82e0d67ba4475093ddbbbaa63e678b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.3-class.exclusive.seed_66 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 66), ('class_num', 5), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '5-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761a4906f30349958ac1de407abf9823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_66 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 77), ('class_num', 2), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '2-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0323ef7feeee4c5c91ae94bc42cd34d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "Dropped 391 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_77 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 72/72 [00:14<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 77), ('class_num', 3), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '3-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fa9950fc494fdab38bd90486481f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.3-class.exclusive.seed_77 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 77), ('class_num', 5), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '5-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022cb8eb337c4b7bbe016edb4495dcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_77 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 88), ('class_num', 2), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '2-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bba1620f6a14cc6b819dc02780bc6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "Dropped 391 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_88 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 72/72 [00:14<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 88), ('class_num', 3), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '3-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f2e4a9057b4c03bc6089184f15832b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.3-class.exclusive.seed_88 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 88), ('class_num', 5), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '5-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d1b0f486ce4d65909c6f86fe1e33c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_88 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 99), ('class_num', 2), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '2-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d6cf12cb934c5b9b1372059599a7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "Dropped 391 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_99 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 72/72 [00:14<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 99), ('class_num', 3), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '3-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d60fd0c9454d9694e22d1d252bddcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.3-class.exclusive.seed_99 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for this setting:  (('seed', 99), ('class_num', 5), ('beta', 1.0), ('gemma', 3.0), ('h_dim', 192), ('dataset_type', '5-way'), ('correction_epsilon', None), ('cls_dropout', 0.1), ('enc_dropout', 0.1), ('control', True), ('model_arch', 'bert-base-uncased'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4172b3e31b1a4cefb503b778d421fff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n",
      "intervention_h_dim=192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_99 and are newly initialized: ['multitask_classifier.dense.bias', 'multitask_classifier.out_proj.weight', 'multitask_classifier.out_proj.bias', 'multitask_classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:25<00:00,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in range(len(permutations_dicts)):\n",
    "    \n",
    "    seed=permutations_dicts[i][\"seed\"]\n",
    "    class_num=permutations_dicts[i][\"class_num\"]\n",
    "    beta=permutations_dicts[i][\"beta\"]\n",
    "    gemma=permutations_dicts[i][\"gemma\"]\n",
    "    h_dim=permutations_dicts[i][\"h_dim\"]\n",
    "    dataset_type = f'{class_num}-way'\n",
    "    correction_epsilon=None\n",
    "    cls_dropout=permutations_dicts[i][\"cls_dropout\"]\n",
    "    enc_dropout=permutations_dicts[i][\"enc_dropout\"]\n",
    "    control=permutations_dicts[i][\"control\"]\n",
    "    model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "    \n",
    "    if model_arch == \"bert-base-uncased\":\n",
    "        model_path = \"BERT-control-results\" if control else \"BERT-results\"\n",
    "        model_module = BERTForCEBaB\n",
    "        explainer_module = CausalProxyModelForBERT\n",
    "    elif model_arch == \"roberta-base\":\n",
    "        model_path = \"RoBERTa-control-results\" if control else \"RoBERTa-results\"\n",
    "    elif model_arch == \"gpt2\":\n",
    "        model_path = \"gpt2-control-results\" if control else \"gpt2-results\"\n",
    "    elif model_arch == \"lstm\":\n",
    "        model_path = \"lstm-control-results\" if control else \"lstm-results\"\n",
    "        \n",
    "    grid_conditions=(\n",
    "        (\"seed\", seed),\n",
    "        (\"class_num\", class_num),\n",
    "        (\"beta\", beta),\n",
    "        (\"gemma\", gemma),\n",
    "        (\"h_dim\", h_dim),\n",
    "        (\"dataset_type\", dataset_type),\n",
    "        (\"correction_epsilon\", correction_epsilon),\n",
    "        (\"cls_dropout\", cls_dropout),\n",
    "        (\"enc_dropout\", enc_dropout),\n",
    "        (\"control\", control),\n",
    "        (\"model_arch\", model_arch),\n",
    "    )\n",
    "    print(\"Running for this setting: \", grid_conditions)\n",
    "\n",
    "    blackbox_model_path = f'CEBaB/{model_arch}.CEBaB.sa.{class_num}-class.exclusive.seed_{seed}'\n",
    "    if control:\n",
    "        cpm_model_path = blackbox_model_path\n",
    "    else:\n",
    "        cpm_model_path = f'../proxy_training_results/{model_path}/'\\\n",
    "                           f'cebab.train.train.alpha.1.0'\\\n",
    "                           f'.beta.{beta}.gemma.{gemma}.dim.{h_dim}.hightype.'\\\n",
    "                           f'{model_arch}.Proxy.'\\\n",
    "                           f'CEBaB.sa.{class_num}-class.exclusive.'\\\n",
    "                           f'mode.align.cls.dropout.{cls_dropout}.enc.dropout.{enc_dropout}.seed_{seed}'\n",
    "\n",
    "    # load data from HF\n",
    "    cebab = datasets.load_dataset(\n",
    "        'CEBaB/CEBaB', use_auth_token=True,\n",
    "        cache_dir=\"../../huggingface_cache/\"\n",
    "    )\n",
    "    train, dev, test = preprocess_hf_dataset(\n",
    "        cebab, one_example_per_world=True, \n",
    "        verbose=1, dataset_type=dataset_type\n",
    "    )\n",
    "\n",
    "    tf_model = model_module(\n",
    "        blackbox_model_path, \n",
    "        device=device, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    explanator = explainer_module(\n",
    "        blackbox_model_path,\n",
    "        cpm_model_path, \n",
    "        device=device, \n",
    "        batch_size=batch_size,\n",
    "        intervention_h_dim=h_dim,\n",
    "    )\n",
    "\n",
    "    train_dataset = train.copy()\n",
    "    dev_dataset = test.copy()\n",
    "\n",
    "    result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "    CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "    ACaCE_per_aspect, performance_report = cebab_pipeline(\n",
    "        tf_model, explanator, \n",
    "        train_dataset, dev_dataset, \n",
    "        dataset_type=dataset_type,\n",
    "        correction_epsilon=correction_epsilon,\n",
    "    )\n",
    "    \n",
    "    results[grid_conditions] = (\n",
    "        result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "        CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "        ACaCE_per_aspect, performance_report\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabularize your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>h_dim</th>\n",
       "      <th>class_num</th>\n",
       "      <th>control</th>\n",
       "      <th>beta</th>\n",
       "      <th>gemma</th>\n",
       "      <th>cls_dropout</th>\n",
       "      <th>enc_dropout</th>\n",
       "      <th>model_arch</th>\n",
       "      <th>ICaCE-L2</th>\n",
       "      <th>ICaCE-cosine</th>\n",
       "      <th>ICaCE-normdiff</th>\n",
       "      <th>macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.976104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.7912</td>\n",
       "      <td>0.2962</td>\n",
       "      <td>0.967875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.974483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>0.2886</td>\n",
       "      <td>0.971180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>99</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.2894</td>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.976129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.5235</td>\n",
       "      <td>0.812163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.5434</td>\n",
       "      <td>0.7427</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.824431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.7484</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.820531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>88</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.5379</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.809057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>99</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.5458</td>\n",
       "      <td>0.7164</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.824533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.6709</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.696461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.7139</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>0.693366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>0.7209</td>\n",
       "      <td>0.7047</td>\n",
       "      <td>0.705158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>88</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.7213</td>\n",
       "      <td>0.696492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>99</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.7836</td>\n",
       "      <td>0.6881</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.687561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed  h_dim  class_num  control  beta  gemma  cls_dropout  enc_dropout  \\\n",
       "0     42    192          2     True   1.0    3.0          0.1          0.1   \n",
       "3     66    192          2     True   1.0    3.0          0.1          0.1   \n",
       "6     77    192          2     True   1.0    3.0          0.1          0.1   \n",
       "9     88    192          2     True   1.0    3.0          0.1          0.1   \n",
       "12    99    192          2     True   1.0    3.0          0.1          0.1   \n",
       "1     42    192          3     True   1.0    3.0          0.1          0.1   \n",
       "4     66    192          3     True   1.0    3.0          0.1          0.1   \n",
       "7     77    192          3     True   1.0    3.0          0.1          0.1   \n",
       "10    88    192          3     True   1.0    3.0          0.1          0.1   \n",
       "13    99    192          3     True   1.0    3.0          0.1          0.1   \n",
       "2     42    192          5     True   1.0    3.0          0.1          0.1   \n",
       "5     66    192          5     True   1.0    3.0          0.1          0.1   \n",
       "8     77    192          5     True   1.0    3.0          0.1          0.1   \n",
       "11    88    192          5     True   1.0    3.0          0.1          0.1   \n",
       "14    99    192          5     True   1.0    3.0          0.1          0.1   \n",
       "\n",
       "           model_arch  ICaCE-L2  ICaCE-cosine  ICaCE-normdiff  macro-f1  \n",
       "0   bert-base-uncased    0.2876        0.8078          0.2860  0.976104  \n",
       "3   bert-base-uncased    0.2977        0.7912          0.2962  0.967875  \n",
       "6   bert-base-uncased    0.2908        0.7908          0.2900  0.974483  \n",
       "9   bert-base-uncased    0.2916        0.8012          0.2886  0.971180  \n",
       "12  bert-base-uncased    0.2894        0.8047          0.2879  0.976129  \n",
       "1   bert-base-uncased    0.5499        0.7396          0.5235  0.812163  \n",
       "4   bert-base-uncased    0.5434        0.7427          0.5217  0.824431  \n",
       "7   bert-base-uncased    0.5585        0.7484          0.5334  0.820531  \n",
       "10  bert-base-uncased    0.5379        0.7273          0.5176  0.809057  \n",
       "13  bert-base-uncased    0.5458        0.7164          0.5284  0.824533  \n",
       "2   bert-base-uncased    0.7985        0.6709          0.7273  0.696461  \n",
       "5   bert-base-uncased    0.7666        0.7139          0.6560  0.693366  \n",
       "8   bert-base-uncased    0.7992        0.7209          0.7047  0.705158  \n",
       "11  bert-base-uncased    0.8136        0.7152          0.7213  0.696492  \n",
       "14  bert-base-uncased    0.7836        0.6881          0.6988  0.687561  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_keys = list(grid.keys())\n",
    "values = []\n",
    "for k, v in results.items():\n",
    "    _values = []\n",
    "    for ik in important_keys:\n",
    "        _values.append(dict(k)[ik])\n",
    "    _values.append(v[2][\"ICaCE-L2\"].iloc[0])\n",
    "    _values.append(v[2][\"ICaCE-cosine\"].iloc[0])\n",
    "    _values.append(v[2][\"ICaCE-normdiff\"].iloc[0])\n",
    "    _values.append(v[-1].iloc[0][0])\n",
    "    values.append(_values)\n",
    "important_keys.extend([\"ICaCE-L2\", \"ICaCE-cosine\", \"ICaCE-normdiff\", \"macro-f1\"])\n",
    "df = pd.DataFrame(values, columns=important_keys)\n",
    "df.sort_values(by=['class_num'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your results somewhere and load again to tabularize your results altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plase give an output file name: results\n",
      "Writing to file:  ../proxy_training_results/BERT-control-results/results.pkl\n"
     ]
    }
   ],
   "source": [
    "output_name = input(\"Plase give an output file name: \")\n",
    "\n",
    "output_directory = f'../proxy_training_results/{model_path}/'\n",
    "output_filename = os.path.join(output_directory, f'{output_name}.pkl')\n",
    "print(\"Writing to file: \", output_filename)\n",
    "with open(output_filename, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
