{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Mediation Analyses\n",
    "Static version of activate alignment search without interchange intervention training. This analysis follows closely the [causal mediation analyses paper](https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf). We simplify their method to adapt to CEBaB. Details can be found in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *\n",
    "\n",
    "\"\"\"\n",
    "For evaluate, we use a single random seed, as\n",
    "the models are trained with 5 different seeds\n",
    "already.\n",
    "\"\"\"\n",
    "_ = random.seed(123)\n",
    "_ = np.random.seed(123)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../train_cache/CEBaB___parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fd2f3550b343d59ed5eb2297661ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.6382% of train dataset.\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "class_num=5\n",
    "beta=1.0\n",
    "gemma=3.0\n",
    "h_dim=4\n",
    "dataset_type = f'{class_num}-way'\n",
    "correction_epsilon=None\n",
    "cls_dropout=0.1\n",
    "enc_dropout=0.1\n",
    "control=False\n",
    "model_arch=\"bert-base-uncased\"\n",
    "neuron_pool_size = h_dim*4\n",
    "\n",
    "# these are the control settings.\n",
    "concepts = [\"ambiance\", \"food\", \"noise\", \"service\"]\n",
    "random_align_neurons = {}\n",
    "random_neuron_pool = [i for i in range(neuron_pool_size)]\n",
    "random.shuffle(random_neuron_pool)\n",
    "for idx, c in enumerate(concepts):\n",
    "    random_align_neurons[c] = set(random_neuron_pool[idx*h_dim:(idx+1)*h_dim])\n",
    "\n",
    "device='cuda:0'\n",
    "batch_size=32\n",
    "    \n",
    "model_path = f'CEBaB/{model_arch}.CEBaB.sa.{class_num}-class.exclusive.seed_{seed}'\n",
    "\n",
    "# load data from HF\n",
    "cebab = datasets.load_dataset(\n",
    "    'CEBaB/CEBaB', use_auth_token=True,\n",
    "    cache_dir=\"../train_cache/\"\n",
    ")\n",
    "cebab['train'] = cebab['train_exclusive']\n",
    "train, dev, test = preprocess_hf_dataset(\n",
    "    cebab, one_example_per_world=True, \n",
    "    verbose=1, dataset_type=dataset_type\n",
    ")\n",
    "\n",
    "train_dataset = train.copy()\n",
    "dev_dataset = test.copy()\n",
    "\n",
    "tf_model = BERTForCEBaB(\n",
    "    model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved alignment found for this model: ./neuron_alignments/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42.alignment.\n"
     ]
    }
   ],
   "source": [
    "output_filename = os.path.join(\"./neuron_alignments/\", model_path.split(\"/\")[-1]+\".alignment\")\n",
    "if not os.path.isfile(output_filename):\n",
    "    dataset = train_dataset[[\n",
    "        \"description\",\n",
    "        \"ambiance_aspect_majority\",\n",
    "        \"food_aspect_majority\",\n",
    "        \"noise_aspect_majority\",\n",
    "        \"service_aspect_majority\",\n",
    "    ]]\n",
    "    align_neurons = {}\n",
    "    neuron_pool = set([i for i in range(neuron_pool_size)])\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    print(\"preloading representations.\")\n",
    "    # pre-calculating representations to avoid repeated computations.\n",
    "    preload_representations = {}\n",
    "    preload_logits = {}\n",
    "    for index, row in dataset.iterrows():\n",
    "        description = row[\"description\"]\n",
    "        explanator.model.model.eval()\n",
    "        x = explanator.tokenizer([description], padding=True, truncation=True, return_tensors='pt')\n",
    "        x_batch = {k: v.to(explanator.device) for k, v in x.items()}\n",
    "        outputs = explanator.model.model(\n",
    "            **x_batch,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        cls_hidden_state = outputs.hidden_states[-1][:,0,:].detach()\n",
    "        output_logit = torch.nn.functional.softmax(\n",
    "                outputs.logits[0].cpu(), dim=-1\n",
    "        ).detach()[0]\n",
    "        preload_representations[description] = cls_hidden_state\n",
    "        preload_logits[description] = output_logit\n",
    "    \n",
    "    for align_concept in concepts:\n",
    "        print(f\"aligning for concept={align_concept}.\")\n",
    "        align_neurons[align_concept] = set([])\n",
    "        neuron_causal_effect = dict([\n",
    "            (i, 0.0) for i in range(explanator.model.model.config.hidden_size)\n",
    "        ]) # treatment effect / control effect\n",
    "        for index, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "            control_concepts = list(set(concepts) - set([align_concept]))\n",
    "            description = row[\"description\"]\n",
    "            concept_label = row[f\"{align_concept}_aspect_majority\"]\n",
    "            counterfactual_concept_row = dataset[\n",
    "                dataset[f\"{align_concept}_aspect_majority\"]!=concept_label\n",
    "            ].sample().iloc[0]\n",
    "            counterfactual_description = counterfactual_concept_row[\"description\"]\n",
    "            if concept_label != \"\" and counterfactual_concept_row[\n",
    "                f\"{align_concept}_aspect_majority\"\n",
    "            ] != \"\":\n",
    "                # check logit change by looping through all neurons.\n",
    "                for neuron_id in neuron_pool:\n",
    "                    logits = preload_logits[description]\n",
    "                    intervened_logits = intervene_neuron_logits(\n",
    "                        explanator, \n",
    "                        preload_representations[description].clone(), \n",
    "                        preload_representations[counterfactual_description],\n",
    "                        neuron_id,\n",
    "                    )\n",
    "                    te = loss(intervened_logits,logits)\n",
    "                    ce = 0.0\n",
    "                    ce_count = 0\n",
    "                    for control_concept in control_concepts:\n",
    "                        control_label = row[f\"{control_concept}_aspect_majority\"]\n",
    "                        counterfactual_control_concept_row = dataset[\n",
    "                            dataset[f\"{control_concept}_aspect_majority\"]!=control_label\n",
    "                        ].sample().iloc[0]\n",
    "                        counterfactual_control_description = counterfactual_control_concept_row[\n",
    "                            \"description\"\n",
    "                        ]\n",
    "                        if control_label != \"\" and counterfactual_control_concept_row[\n",
    "                            f\"{control_concept}_aspect_majority\"\n",
    "                        ] != \"\":\n",
    "                            intervened_control_logits = intervene_neuron_logits(\n",
    "                                explanator, \n",
    "                                preload_representations[description].clone(), \n",
    "                                preload_representations[counterfactual_control_description],\n",
    "                                neuron_id,\n",
    "                            )\n",
    "                            ce += loss(intervened_control_logits,logits)\n",
    "                            ce_count += 1\n",
    "                    if ce_count != 0:\n",
    "                        ce /= ce_count\n",
    "                        ratio = 1/len(dataset)\n",
    "                        neuron_causal_effect[neuron_id] += (ratio*(te/ce)).tolist()\n",
    "        neuron_causal_effect = sorted(neuron_causal_effect.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_neuron = set([\n",
    "            neuron_causal_effect[i][0] for i in range(int(explanator.intervention_h_dim))\n",
    "        ])\n",
    "        align_neurons[align_concept] = selected_neuron\n",
    "        # remove from avaliable pool.\n",
    "        neuron_pool -= selected_neuron\n",
    "        # we may stop early, if we ask to align all neurons for this repr.\n",
    "        if len(neuron_pool) == int(explanator.intervention_h_dim):\n",
    "            print(\"since mapping all neurons, skip the last one to take all remaining neurons.\")\n",
    "            align_neurons[concepts[-1]] = neuron_pool\n",
    "            break\n",
    "\n",
    "    # save the alignment for future use.\n",
    "    serialized_align_neurons = {}\n",
    "    for k, v in align_neurons.items():\n",
    "        serialized_align_neurons[k] = list(v)\n",
    "\n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        json.dump(serialized_align_neurons, outfile, indent=4)\n",
    "else:\n",
    "    print(f\"saved alignment found for this model: {output_filename}.\")\n",
    "    align_neurons = json.load(open(output_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with neurons selected by causal mediation analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.dense.weight', 'multitask_classifier.out_proj.weight', 'multitask_classifier.dense.bias', 'multitask_classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:20<00:00,  6.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ICaCE-L2</th>\n",
       "      <th>ICaCE-cosine</th>\n",
       "      <th>ICaCE-normdiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42</th>\n",
       "      <th>CausalMediationModelForBERT</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.7922</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>0.7169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     ICaCE-L2  \\\n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean    0.7922   \n",
       "\n",
       "                                                                                     ICaCE-cosine  \\\n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean        0.6726   \n",
       "\n",
       "                                                                                     ICaCE-normdiff  \n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean          0.7169  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = CausalMediationModelForBERT(\n",
    "    model_path,\n",
    "    device=device, \n",
    "    batch_size=batch_size,\n",
    "    intervention_h_dim=h_dim,\n",
    "    align_neurons=align_neurons\n",
    ")\n",
    "\n",
    "result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "ACaCE_per_aspect, performance_report = cebab_pipeline(\n",
    "    tf_model, explainer, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type=\"5-way\",\n",
    "    correction_epsilon=None,\n",
    ")\n",
    "\n",
    "CEBaB_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with neurons that are randomly selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IITBERTForSequenceClassification were not initialized from the model checkpoint at CEBaB/bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42 and are newly initialized: ['multitask_classifier.dense.weight', 'multitask_classifier.out_proj.weight', 'multitask_classifier.dense.bias', 'multitask_classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 124/124 [00:12<00:00,  9.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ICaCE-L2</th>\n",
       "      <th>ICaCE-cosine</th>\n",
       "      <th>ICaCE-normdiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased.CEBaB.sa.5-class.exclusive.seed_42</th>\n",
       "      <th>CausalMediationModelForBERT</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.8135</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.8108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     ICaCE-L2  \\\n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean    0.8135   \n",
       "\n",
       "                                                                                     ICaCE-cosine  \\\n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean         0.849   \n",
       "\n",
       "                                                                                     ICaCE-normdiff  \n",
       "bert-base-uncased.CEBaB.sa.5-class.exclusive.se... CausalMediationModelForBERT mean          0.8108  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_explainer = CausalMediationModelForBERT(\n",
    "    model_path,\n",
    "    device=device, \n",
    "    batch_size=batch_size,\n",
    "    intervention_h_dim=h_dim,\n",
    "    align_neurons=random_align_neurons\n",
    ")\n",
    "\n",
    "result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "ACaCE_per_aspect, performance_report = cebab_pipeline(\n",
    "    tf_model, control_explainer, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type=\"5-way\",\n",
    "    correction_epsilon=None,\n",
    ")\n",
    "\n",
    "CEBaB_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
