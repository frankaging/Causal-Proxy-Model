{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from modelings.modelings_bert import *\n",
    "from modelings.modelings_roberta import *\n",
    "from modelings.modelings_gpt2 import *\n",
    "from modelings.modelings_lstm import *\n",
    "\"\"\"\n",
    "For evaluate, we use a single random seed, as\n",
    "the models are trained with 5 different seeds\n",
    "already.\n",
    "\"\"\"\n",
    "_ = random.seed(123)\n",
    "_ = np.random.seed(123)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "# rerunning the boxes below will only append stuffs to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following blocks will run CEBaB benchmark in\n",
    "all the combinations of the following conditions.\n",
    "\"\"\"\n",
    "grid = {\n",
    "    \"eval_split\": [\"dev\"],\n",
    "    # dev,test\n",
    "    \"control\": [\"checkpoints\"],\n",
    "    # baseline-random,baseline-blackbox,hdims,layers,ks,approximate,ablation,checkpoints\n",
    "    \"seed\": [42],\n",
    "    # 42, 66, 77\n",
    "    \"h_dim\": [192],\n",
    "    # 1,16,64,128,192\n",
    "    # 1,16,64,75\n",
    "    \"interchange_layer\" : [10],\n",
    "    # 0,1; 2,4,6,8,10,12\n",
    "    \"class_num\": [5],\n",
    "    \"k\" : [19684], \n",
    "    # 0;10,100,500,1000,3000,6000,9848,19684\n",
    "    \"alpha\" : [1.0],\n",
    "    # 0.0,1.0\n",
    "    \"beta\" : [1.0],\n",
    "    # 0.0,1.0\n",
    "    \"gemma\" : [3.0],\n",
    "    # 0.0,3.0\n",
    "    \"model_arch\" : [\"bert-base-uncased\"],\n",
    "    # lstm, bert-base-uncased, roberta-base, gpt2\n",
    "    \"lr\" : [\"8e-05\"],\n",
    "    # 8e-05; 0.001\n",
    "    \"counterfactual_type\" : [\"true\"]\n",
    "    # approximate,true\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "device = 'cuda:2'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "eval_split=permutations_dicts[i][\"eval_split\"]\n",
    "seed=permutations_dicts[i][\"seed\"]\n",
    "class_num=permutations_dicts[i][\"class_num\"]\n",
    "alpha=permutations_dicts[i][\"alpha\"]\n",
    "beta=permutations_dicts[i][\"beta\"]\n",
    "gemma=permutations_dicts[i][\"gemma\"]\n",
    "h_dim=permutations_dicts[i][\"h_dim\"]\n",
    "dataset_type = f'{class_num}-way'\n",
    "control=permutations_dicts[i][\"control\"]\n",
    "model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "k=permutations_dicts[i][\"k\"]\n",
    "interchange_layer=permutations_dicts[i][\"interchange_layer\"]\n",
    "lr=permutations_dicts[i][\"lr\"]\n",
    "counterfactual_type=permutations_dicts[i][\"counterfactual_type\"]\n",
    "\n",
    "if model_arch == \"bert-base-uncased\":\n",
    "    model_path = \"BERT\"\n",
    "    model_module = BERTForCEBaB\n",
    "    explainer_module = CausalProxyModelForBERT\n",
    "elif model_arch == \"roberta-base\":\n",
    "    model_path = \"RoBERTa\" \n",
    "    model_module = RoBERTaForCEBaB\n",
    "    explainer_module = CausalProxyModelForRoBERTa\n",
    "elif model_arch == \"gpt2\":\n",
    "    model_path = \"gpt2\"\n",
    "    model_module = GPT2ForCEBaB\n",
    "    explainer_module = CausalProxyModelForGPT2\n",
    "elif model_arch == \"lstm\":\n",
    "    model_path = \"lstm\"\n",
    "    model_module = LSTMForCEBaB\n",
    "    explainer_module = CausalProxyModelForLSTM\n",
    "model_path += f\"-{control}\"\n",
    "\n",
    "\n",
    "for j in range(0, 7):\n",
    "    \n",
    "    grid_conditions=(\n",
    "        (\"eval_split\", eval_split),\n",
    "        (\"control\", control),\n",
    "        (\"seed\", seed),\n",
    "        (\"h_dim\", h_dim),\n",
    "        (\"interchange_layer\", interchange_layer),\n",
    "        (\"class_num\", class_num),\n",
    "        (\"k\", k),\n",
    "        (\"alpha\", alpha),\n",
    "        (\"beta\", beta),\n",
    "        (\"gemma\", gemma),\n",
    "        (\"model_arch\", model_arch),\n",
    "        (\"lr\", lr),\n",
    "        (\"counterfactual_type\", counterfactual_type),\n",
    "        (\"checkpoint\", j)\n",
    "    )\n",
    "    print(\"Running for this setting: \", grid_conditions)\n",
    "    \n",
    "    blackbox_model_path = f'CEBaB/{model_arch}.CEBaB.sa.'\\\n",
    "                          f'{class_num}-class.exclusive.seed_{seed}'\n",
    "    cpm_model_path = f'../proxy_training_results/{model_path}/'\\\n",
    "                     f'cebab.alpha.{alpha}.beta.{beta}.gemma.{gemma}.'\\\n",
    "                     f'lr.{lr}.dim.{h_dim}.hightype.{model_arch}.'\\\n",
    "                     f'CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.'\\\n",
    "                     f'{counterfactual_type}.k.{k}.int.layer.{interchange_layer}.'\\\n",
    "                     f'seed_{seed}/checkpoint-{j}'\n",
    "\n",
    "    # load data from HF\n",
    "    cebab = datasets.load_dataset(\n",
    "        'CEBaB/CEBaB', use_auth_token=True,\n",
    "        cache_dir=\"../train_cache/\"\n",
    "    )\n",
    "\n",
    "    train, dev, test = preprocess_hf_dataset_inclusive(\n",
    "        cebab, verbose=1, dataset_type=dataset_type\n",
    "    )\n",
    "\n",
    "    eval_dataset = dev if eval_split == 'dev' else test\n",
    "\n",
    "    tf_model = model_module(\n",
    "        blackbox_model_path, \n",
    "        device=device, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    explainer = explainer_module(\n",
    "        blackbox_model_path,\n",
    "        cpm_model_path, \n",
    "        device=device, \n",
    "        batch_size=batch_size,\n",
    "        intervention_h_dim=h_dim,\n",
    "    )\n",
    "    \n",
    "    result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "    CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "    ACaCE_per_aspect, performance_report = cebab_pipeline(\n",
    "        tf_model, explainer, \n",
    "        train, eval_dataset,\n",
    "        seed, k, dataset_type=dataset_type, \n",
    "        shorten_model_name=False, \n",
    "        train_setting=\"inclusive\", \n",
    "        approximate=False if counterfactual_type == \"true\" else True\n",
    "    )\n",
    "    \n",
    "    results[grid_conditions] = (\n",
    "        result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "        CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "        ACaCE_per_aspect, performance_report\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keys = [\n",
    "    \"eval_split\", \"seed\", \"checkpoint\"\n",
    "]\n",
    "values = []\n",
    "for k, v in results.items():\n",
    "    _values = []\n",
    "    for ik in important_keys:\n",
    "        _values.append(dict(k)[ik])\n",
    "    _values.append(v[2][\"ICaCE-L2\"].iloc[0])\n",
    "    _values.append(v[2][\"ICaCE-cosine\"].iloc[0])\n",
    "    _values.append(v[2][\"ICaCE-normdiff\"].iloc[0])\n",
    "    _values.append(v[-1].iloc[0][0])\n",
    "    values.append(_values)\n",
    "important_keys.extend([\"ICaCE-L2\", \"ICaCE-cosine\", \"ICaCE-normdiff\", \"macro-f1\"])\n",
    "df = pd.DataFrame(values, columns=important_keys)\n",
    "df.sort_values(by=['checkpoint'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_plot(ax, x, y, y_std, color, marker, ylims, label=None):\n",
    "    ax.plot(x, y, color=color, marker=marker, markersize=12, label=label)\n",
    "    ax.set_ylim(ylims[0], ylims[1])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"checkpoint\"]!=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = df[df[\"eval_split\"] == \"dev\"]\n",
    "test_df = df[df[\"eval_split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "items = [\"ICaCE-L2\", \"ICaCE-cosine\", \"ICaCE-normdiff\", \"macro-f1\"]\n",
    "x = test_df[\"checkpoint\"]+1\n",
    "x = x.tolist()\n",
    "\n",
    "with plt.rc_context({\n",
    "    'axes.edgecolor':'black', 'xtick.color':'black', \n",
    "    'ytick.color':'black', 'axes.facecolor':'white', \n",
    "}):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 3))\n",
    "    axs[0].set_title('L2', fontsize=20)\n",
    "    axs[1].set_title('Cosine', fontsize=20)\n",
    "    axs[2].set_title('NormDiff', fontsize=20)\n",
    "    axs[3].set_title('Macro-F1', fontsize=20)\n",
    "    \n",
    "    # layers\n",
    "    ylims_all = [\n",
    "        [0.35, 0.75],\n",
    "        [0.15, 0.55],\n",
    "        [0.20, 0.60],\n",
    "        [0.55, 0.95]\n",
    "    ]\n",
    "    \n",
    "    # '#377eb8', '#ff7f00', '#4daf4a', '#f781bf'\n",
    "    \n",
    "    make_single_plot(\n",
    "        axs[0], x, dev_df[\"ICaCE-L2\"].tolist(), [], \n",
    "        color='#4daf4a', marker=\"^\", ylims=ylims_all[0], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[1], x, dev_df[\"ICaCE-cosine\"].tolist(), [], \n",
    "        color='#4daf4a', marker=\"^\", ylims=ylims_all[1], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[2], x, dev_df[\"ICaCE-normdiff\"].tolist(), [], \n",
    "        color='#4daf4a', marker=\"^\", ylims=ylims_all[2], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[3], x, dev_df[\"macro-f1\"].tolist(), [], \n",
    "        color='#4daf4a', marker=\"^\", ylims=ylims_all[3], label=\"dev\"\n",
    "    )\n",
    "    \n",
    "    make_single_plot(\n",
    "        axs[0], x, test_df[\"ICaCE-L2\"].tolist(), [], \n",
    "        color='#ff7f00', marker=\"*\", ylims=ylims_all[0], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[1], x, test_df[\"ICaCE-cosine\"].tolist(), [], \n",
    "        color='#ff7f00', marker=\"*\", ylims=ylims_all[1], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[2], x, test_df[\"ICaCE-normdiff\"].tolist(), [], \n",
    "        color='#ff7f00', marker=\"*\", ylims=ylims_all[2], label=None\n",
    "    )\n",
    "    make_single_plot(\n",
    "        axs[3], x, test_df[\"macro-f1\"].tolist(), [], \n",
    "        color='#ff7f00', marker=\"*\", ylims=ylims_all[3], label=\"test\"\n",
    "    )\n",
    "    \n",
    "    axs[3].legend(loc='upper right',\n",
    "                 ncol=1, fancybox=True, shadow=True, fontsize=12, facecolor=\"white\")\n",
    "        \n",
    "    _ = fig.text(0.5, -0.08, r'Epoch Number', ha='center', fontsize=22)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.2, \n",
    "                        hspace=0.2)\n",
    "    plt.savefig(\"./figures/metrics-epoch.png\",dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import functools\n",
    "import warnings\n",
    "from typing import Any, Callable, List, overload, Tuple, Union\n",
    "\n",
    "from captum._utils.common import (\n",
    "    _extract_device,\n",
    "    _format_additional_forward_args,\n",
    "    _format_outputs,\n",
    ")\n",
    "from captum._utils.gradient import _forward_layer_eval #, _run_forward\n",
    "#################### IMPORT MODULES ###########################\n",
    "from inspect import signature\n",
    "from captum._utils.common import _select_targets, _format_input\n",
    "###############################################################\n",
    "from captum._utils.typing import BaselineType, Literal, ModuleOrModuleList, TargetType\n",
    "from captum.attr._core.integrated_gradients import IntegratedGradients\n",
    "from captum.attr._utils.attribution import GradientAttribution, LayerAttribution\n",
    "from captum.attr._utils.common import (\n",
    "    _format_input_baseline,\n",
    "    _tensorize_baseline,\n",
    "    _validate_input,\n",
    ")\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor\n",
    "from torch.nn.parallel.scatter_gather import scatter\n",
    "\n",
    "############### A FEW CONSTANTS (SHOULD LATER INTEGRATE AS ARGUMENTS) ####################\n",
    "INTERVENTION_H_DIM = 192\n",
    "MODEL_H_DIM = 768\n",
    "INTERCHANGE_LAYER = interchange_layer\n",
    "CONTROL = False\n",
    "\n",
    "####################### OVERRIDE RUN FORWARD FUNCTION ####################################\n",
    "def _run_forward(\n",
    "    forward_func: Callable,\n",
    "    inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "    target: TargetType = None,\n",
    "    additional_forward_args: Any = None,\n",
    ") -> Tensor:\n",
    "    forward_func_args = signature(forward_func).parameters\n",
    "    if len(forward_func_args) == 0:\n",
    "        output = forward_func()\n",
    "        return output if target is None else _select_targets(output, target)\n",
    "\n",
    "    # make everything a tuple so that it is easy to unpack without\n",
    "    # using if-statements\n",
    "    inputs = _format_input(inputs)\n",
    "    additional_forward_args = _format_additional_forward_args(additional_forward_args)\n",
    "\n",
    "    output = forward_func(\n",
    "        *(*inputs, *additional_forward_args)\n",
    "        if additional_forward_args is not None\n",
    "        else inputs,\n",
    "    )\n",
    "    if CONTROL:\n",
    "        logits = output.logits # take first logit for overall sentiment\n",
    "    else:\n",
    "        logits = output.logits[0] # take first logit for overall sentiment\n",
    "    final_activation = output.hidden_states[INTERCHANGE_LAYER]\n",
    "    return _select_targets(logits, target), final_activation\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "class LayerIntegratedGradients(LayerAttribution, GradientAttribution):\n",
    "    r\"\"\"\n",
    "    Layer Integrated Gradients is a variant of Integrated Gradients that assigns\n",
    "    an importance score to layer inputs or outputs, depending on whether we\n",
    "    attribute to the former or to the latter one.\n",
    "\n",
    "    Integrated Gradients is an axiomatic model interpretability algorithm that\n",
    "    attributes / assigns an importance score to each input feature by approximating\n",
    "    the integral of gradients of the model's output with respect to the inputs\n",
    "    along the path (straight line) from given baselines / references to inputs.\n",
    "\n",
    "    Baselines can be provided as input arguments to attribute method.\n",
    "    To approximate the integral we can choose to use either a variant of\n",
    "    Riemann sum or Gauss-Legendre quadrature rule.\n",
    "\n",
    "    More details regarding the integrated gradients method can be found in the\n",
    "    original paper:\n",
    "    https://arxiv.org/abs/1703.01365\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable,\n",
    "        layer: ModuleOrModuleList,\n",
    "        device_ids: Union[None, List[int]] = None,\n",
    "        multiply_by_inputs: bool = True,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            forward_func (callable):  The forward function of the model or any\n",
    "                        modification of it\n",
    "            layer (ModuleOrModuleList):\n",
    "                        Layer or list of layers for which attributions are computed.\n",
    "                        For each layer the output size of the attribute matches\n",
    "                        this layer's input or output dimensions, depending on\n",
    "                        whether we attribute to the inputs or outputs of the\n",
    "                        layer, corresponding to the attribution of each neuron\n",
    "                        in the input or output of this layer.\n",
    "\n",
    "                        Please note that layers to attribute on cannot be\n",
    "                        dependent on each other. That is, a subset of layers in\n",
    "                        `layer` cannot produce the inputs for another layer.\n",
    "\n",
    "                        For example, if your model is of a simple linked-list\n",
    "                        based graph structure (think nn.Sequence), e.g. x -> l1\n",
    "                        -> l2 -> l3 -> output. If you pass in any one of those\n",
    "                        layers, you cannot pass in another due to the\n",
    "                        dependence, e.g.  if you pass in l2 you cannot pass in\n",
    "                        l1 or l3.\n",
    "\n",
    "            device_ids (list(int)): Device ID list, necessary only if forward_func\n",
    "                        applies a DataParallel model. This allows reconstruction of\n",
    "                        intermediate outputs from batched results across devices.\n",
    "                        If forward_func is given as the DataParallel model itself,\n",
    "                        then it is not necessary to provide this argument.\n",
    "            multiply_by_inputs (bool, optional): Indicates whether to factor\n",
    "                        model inputs' multiplier in the final attribution scores.\n",
    "                        In the literature this is also known as local vs global\n",
    "                        attribution. If inputs' multiplier isn't factored in,\n",
    "                        then this type of attribution method is also called local\n",
    "                        attribution. If it is, then that type of attribution\n",
    "                        method is called global.\n",
    "                        More detailed can be found here:\n",
    "                        https://arxiv.org/abs/1711.06104\n",
    "\n",
    "                        In case of layer integrated gradients, if `multiply_by_inputs`\n",
    "                        is set to True, final sensitivity scores are being multiplied by\n",
    "                        layer activations for inputs - layer activations for baselines.\n",
    "\n",
    "        \"\"\"\n",
    "        LayerAttribution.__init__(self, forward_func, layer, device_ids=device_ids)\n",
    "        GradientAttribution.__init__(self, forward_func)\n",
    "        self.ig = IntegratedGradients(forward_func, multiply_by_inputs)\n",
    "\n",
    "        if isinstance(layer, list) and len(layer) > 1:\n",
    "            warnings.warn(\n",
    "                \"Multiple layers provided. Please ensure that each layer is\"\n",
    "                \"**not** solely solely dependent on the outputs of\"\n",
    "                \"another layer. Please refer to the documentation for more\"\n",
    "                \"detail.\"\n",
    "            )\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType,\n",
    "        target: TargetType,\n",
    "        additional_forward_args: Any,\n",
    "        n_steps: int,\n",
    "        method: str,\n",
    "        internal_batch_size: Union[None, int],\n",
    "        return_convergence_delta: Literal[False],\n",
    "        attribute_to_layer_input: bool,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType,\n",
    "        target: TargetType,\n",
    "        additional_forward_args: Any,\n",
    "        n_steps: int,\n",
    "        method: str,\n",
    "        internal_batch_size: Union[None, int],\n",
    "        return_convergence_delta: Literal[True],\n",
    "        attribute_to_layer_input: bool,\n",
    "    ) -> Tuple[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tensor,\n",
    "    ]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Any = None,\n",
    "        n_steps: int = 50,\n",
    "        method: str = \"gausslegendre\",\n",
    "        internal_batch_size: Union[None, int] = None,\n",
    "        return_convergence_delta: bool = False,\n",
    "        attribute_to_layer_input: bool = False,\n",
    "    ) -> Union[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tuple[\n",
    "            Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "            Tensor,\n",
    "        ],\n",
    "    ]:\n",
    "        ...\n",
    "\n",
    "    @log_usage()\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Any = None,\n",
    "        n_steps: int = 50,\n",
    "        method: str = \"gausslegendre\",\n",
    "        internal_batch_size: Union[None, int] = None,\n",
    "        return_convergence_delta: bool = False,\n",
    "        attribute_to_layer_input: bool = False,\n",
    "        ############## NEW ARGUMENT: ASPECT OF INTEREST ############################\n",
    "        aspect: Union[int, None] = None\n",
    "        ############################################################################\n",
    "    ) -> Union[\n",
    "        Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "        Tuple[\n",
    "            Union[Tensor, Tuple[Tensor, ...], List[Union[Tensor, Tuple[Tensor, ...]]]],\n",
    "            Tensor,\n",
    "        ],\n",
    "    ]:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to layer inputs or outputs of the model, depending on whether\n",
    "        `attribute_to_layer_input` is set to True or False, using the approach\n",
    "        described above.\n",
    "\n",
    "        In addition to that it also returns, if `return_convergence_delta` is\n",
    "        set to True, integral approximation delta based on the completeness\n",
    "        property of integrated gradients.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (tensor or tuple of tensors):  Input for which layer integrated\n",
    "                        gradients are computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, tensor, tuple of scalars or tensors, optional):\n",
    "                        Baselines define the starting point from which integral\n",
    "                        is computed and can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "                            - either a tensor with matching dimensions to\n",
    "                              corresponding tensor in the inputs' tuple\n",
    "                              or the first dimension is one and the remaining\n",
    "                              dimensions match with the corresponding\n",
    "                              input tensor.\n",
    "                            - or a scalar, corresponding to a tensor in the\n",
    "                              inputs' tuple. This scalar value is broadcasted\n",
    "                              for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` is not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "\n",
    "                        Default: None\n",
    "            target (int, tuple, tensor or list, optional):  Output indices for\n",
    "                        which gradients are computed (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            n_steps (int, optional): The number of steps used by the approximation\n",
    "                        method. Default: 50.\n",
    "            method (string, optional): Method for approximating the integral,\n",
    "                        one of `riemann_right`, `riemann_left`, `riemann_middle`,\n",
    "                        `riemann_trapezoid` or `gausslegendre`.\n",
    "                        Default: `gausslegendre` if no method is provided.\n",
    "            internal_batch_size (int, optional): Divides total #steps * #examples\n",
    "                        data points into chunks of size at most internal_batch_size,\n",
    "                        which are computed (forward / backward passes)\n",
    "                        sequentially. internal_batch_size must be at least equal to\n",
    "                        #examples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain internal_batch_size / num_devices examples.\n",
    "                        If internal_batch_size is None, then all evaluations are\n",
    "                        processed in one batch.\n",
    "                        Default: None\n",
    "            return_convergence_delta (bool, optional): Indicates whether to return\n",
    "                        convergence delta or not. If `return_convergence_delta`\n",
    "                        is set to True convergence delta will be returned in\n",
    "                        a tuple following attributions.\n",
    "                        Default: False\n",
    "            attribute_to_layer_input (bool, optional): Indicates whether to\n",
    "                        compute the attribution with respect to the layer input\n",
    "                        or output. If `attribute_to_layer_input` is set to True\n",
    "                        then the attributions will be computed with respect to\n",
    "                        layer input, otherwise it will be computed with respect\n",
    "                        to layer output.\n",
    "                        Note that currently it is assumed that either the input\n",
    "                        or the output of internal layer, depending on whether we\n",
    "                        attribute to the input or output, is a single tensor.\n",
    "                        Support for multiple tensors will be added later.\n",
    "                        Default: False\n",
    "            Returns:\n",
    "                **attributions** or 2-element tuple of **attributions**, **delta**:\n",
    "                - **attributions** (*tensor*, tuple of *tensors* or tuple of *tensors*):\n",
    "                        Integrated gradients with respect to `layer`'s inputs or\n",
    "                        outputs. Attributions will always be the same size and\n",
    "                        dimensionality as the input or output of the given layer,\n",
    "                        depending on whether we attribute to the inputs or outputs\n",
    "                        of the layer which is decided by the input flag\n",
    "                        `attribute_to_layer_input`.\n",
    "\n",
    "                        For a single layer, attributions are returned in a tuple if\n",
    "                        the layer inputs / outputs contain multiple tensors,\n",
    "                        otherwise a single tensor is returned.\n",
    "\n",
    "                        For multiple layers, attributions will always be\n",
    "                        returned as a list. Each element in this list will be\n",
    "                        equivalent to that of a single layer output, i.e. in the\n",
    "                        case that one layer, in the given layers, inputs / outputs\n",
    "                        multiple tensors: the corresponding output element will be\n",
    "                        a tuple of tensors. The ordering of the outputs will be\n",
    "                        the same order as the layers given in the constructor.\n",
    "                - **delta** (*tensor*, returned if return_convergence_delta=True):\n",
    "                        The difference between the total approximated and true\n",
    "                        integrated gradients. This is computed using the property\n",
    "                        that the total sum of forward_func(inputs) -\n",
    "                        forward_func(baselines) must equal the total sum of the\n",
    "                        integrated gradient.\n",
    "                        Delta is calculated per example, meaning that the number of\n",
    "                        elements in returned delta tensor is equal to the number of\n",
    "                        of examples in inputs.\n",
    "\n",
    "            Examples::\n",
    "\n",
    "                >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,\n",
    "                >>> # and returns an Nx10 tensor of class probabilities.\n",
    "                >>> # It contains an attribute conv1, which is an instance of nn.conv2d,\n",
    "                >>> # and the output of this layer has dimensions Nx12x32x32.\n",
    "                >>> net = ImageClassifier()\n",
    "                >>> lig = LayerIntegratedGradients(net, net.conv1)\n",
    "                >>> input = torch.randn(2, 3, 32, 32, requires_grad=True)\n",
    "                >>> # Computes layer integrated gradients for class 3.\n",
    "                >>> # attribution size matches layer output, Nx12x32x32\n",
    "                >>> attribution = lig.attribute(input, target=3)\n",
    "        \"\"\"\n",
    "        inps, baselines = _format_input_baseline(inputs, baselines)\n",
    "        _validate_input(inps, baselines, n_steps, method)\n",
    "\n",
    "        baselines = _tensorize_baseline(inps, baselines)\n",
    "        additional_forward_args = _format_additional_forward_args(\n",
    "            additional_forward_args\n",
    "        )\n",
    "\n",
    "        def flatten_tuple(tup):\n",
    "            return tuple(\n",
    "                sum((list(x) if isinstance(x, (tuple, list)) else [x] for x in tup), [])\n",
    "            )\n",
    "\n",
    "        if self.device_ids is None:\n",
    "            self.device_ids = getattr(self.forward_func, \"device_ids\", None)\n",
    "\n",
    "        inputs_layer = _forward_layer_eval(\n",
    "            self.forward_func,\n",
    "            inps,\n",
    "            self.layer,\n",
    "            device_ids=self.device_ids,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            attribute_to_layer_input=attribute_to_layer_input,\n",
    "        )\n",
    "\n",
    "        # if we have one output\n",
    "        if not isinstance(self.layer, list):\n",
    "            inputs_layer = (inputs_layer,)\n",
    "\n",
    "        num_outputs = [1 if isinstance(x, Tensor) else len(x) for x in inputs_layer]\n",
    "        num_outputs_cumsum = torch.cumsum(\n",
    "            torch.IntTensor([0] + num_outputs), dim=0  # type: ignore\n",
    "        )\n",
    "        inputs_layer = flatten_tuple(inputs_layer)\n",
    "\n",
    "        baselines_layer = _forward_layer_eval(\n",
    "            self.forward_func,\n",
    "            baselines,\n",
    "            self.layer,\n",
    "            device_ids=self.device_ids,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            attribute_to_layer_input=attribute_to_layer_input,\n",
    "        )\n",
    "        baselines_layer = flatten_tuple(baselines_layer)\n",
    "\n",
    "        # inputs -> these inputs are scaled\n",
    "        def gradient_func(\n",
    "            forward_fn: Callable,\n",
    "            inputs: Union[Tensor, Tuple[Tensor, ...]],\n",
    "            target_ind: TargetType = None,\n",
    "            additional_forward_args: Any = None,\n",
    "        ) -> Tuple[Tensor, ...]:\n",
    "            if self.device_ids is None or len(self.device_ids) == 0:\n",
    "                scattered_inputs = (inputs,)\n",
    "            else:\n",
    "                # scatter method does not have a precise enough return type in its\n",
    "                # stub, so suppress the type warning.\n",
    "                scattered_inputs = scatter(  # type:ignore\n",
    "                    inputs, target_gpus=self.device_ids\n",
    "                )\n",
    "\n",
    "            scattered_inputs_dict = {\n",
    "                scattered_input[0].device: scattered_input\n",
    "                for scattered_input in scattered_inputs\n",
    "            }\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(True):\n",
    "\n",
    "                def layer_forward_hook(\n",
    "                    module, hook_inputs, hook_outputs=None, layer_idx=0\n",
    "                ):\n",
    "                    device = _extract_device(module, hook_inputs, hook_outputs)\n",
    "                    is_layer_tuple = (\n",
    "                        isinstance(hook_outputs, tuple)\n",
    "                        # hook_outputs is None if attribute_to_layer_input == True\n",
    "                        if hook_outputs is not None\n",
    "                        else isinstance(hook_inputs, tuple)\n",
    "                    )\n",
    "\n",
    "                    if is_layer_tuple:\n",
    "                        return scattered_inputs_dict[device][\n",
    "                            num_outputs_cumsum[layer_idx] : num_outputs_cumsum[\n",
    "                                layer_idx + 1\n",
    "                            ]\n",
    "                        ]\n",
    "\n",
    "                    return scattered_inputs_dict[device][num_outputs_cumsum[layer_idx]]\n",
    "\n",
    "                hooks = []\n",
    "                try:\n",
    "\n",
    "                    layers = self.layer\n",
    "                    if not isinstance(layers, list):\n",
    "                        layers = [self.layer]\n",
    "\n",
    "                    for layer_idx, layer in enumerate(layers):\n",
    "                        hook = None\n",
    "                        # TODO:\n",
    "                        # Allow multiple attribute_to_layer_input flags for\n",
    "                        # each layer, i.e. attribute_to_layer_input[layer_idx]\n",
    "                        if attribute_to_layer_input:\n",
    "                            hook = layer.register_forward_pre_hook(\n",
    "                                functools.partial(\n",
    "                                    layer_forward_hook, layer_idx=layer_idx\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            hook = layer.register_forward_hook(\n",
    "                                functools.partial(\n",
    "                                    layer_forward_hook, layer_idx=layer_idx\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                        hooks.append(hook)\n",
    "\n",
    "                    output = _run_forward(\n",
    "                        self.forward_func, tuple(), target_ind, additional_forward_args\n",
    "                    )\n",
    "                finally:\n",
    "                    for hook in hooks:\n",
    "                        if hook is not None:\n",
    "                            hook.remove()\n",
    "\n",
    "                # assert output[0].numel() == 1, (\n",
    "                #     \"Target not provided when necessary, cannot\"\n",
    "                #     \" take gradient with respect to multiple outputs.\"\n",
    "                # )\n",
    "                # torch.unbind(forward_out) is a list of scalar tensor tuples and\n",
    "                # contains batch_size * #steps elements\n",
    "                # grads = torch.autograd.grad(torch.unbind(output), inputs)\n",
    "\n",
    "                ##################### TAKE GRADIENT THROUGH ACTIVATION #########################\n",
    "                model_output, final_layer_activation = output\n",
    "\n",
    "                # get aspect representation\n",
    "                if aspect is not None:\n",
    "                    start_idx = aspect * INTERVENTION_H_DIM\n",
    "                    end_idx = (aspect+1) * INTERVENTION_H_DIM\n",
    "                else:\n",
    "                    start_idx = 0\n",
    "                    # end_idx = model_output.hidden_states[-1].size(2)\n",
    "                    end_idx = MODEL_H_DIM\n",
    "\n",
    "                # compute backwards gradient from output logits to final layer\n",
    "                final_layer_gradient = torch.autograd.grad(\n",
    "                    outputs=torch.unbind(model_output),\n",
    "                    inputs=final_layer_activation,\n",
    "                    retain_graph=True\n",
    "                )\n",
    "\n",
    "                final_layer_gradient = torch.stack(final_layer_gradient).squeeze(0).clone().detach()\n",
    "\n",
    "                # reset gradients ??\n",
    "\n",
    "                # compute backwards gradient from final layer to input layer,\n",
    "                # guided by the gradient from the logits to the final layer\n",
    "                mask = torch.zeros_like(final_layer_activation)\n",
    "                mask[:, 0, start_idx:end_idx] = final_layer_gradient[:, 0, start_idx:end_idx]\n",
    "\n",
    "                grads = torch.autograd.grad(\n",
    "                    outputs=torch.unbind(final_layer_activation),\n",
    "                    inputs=inputs,\n",
    "                    grad_outputs=torch.unbind(mask)\n",
    "                )\n",
    "                ###############################################################################\n",
    "\n",
    "            return grads\n",
    "\n",
    "        self.ig.gradient_func = gradient_func\n",
    "        all_inputs = (\n",
    "            (inps + additional_forward_args)\n",
    "            if additional_forward_args is not None\n",
    "            else inps\n",
    "        )\n",
    "\n",
    "        attributions = self.ig.attribute.__wrapped__(  # type: ignore\n",
    "            self.ig,  # self\n",
    "            inputs_layer,\n",
    "            baselines=baselines_layer,\n",
    "            target=target,\n",
    "            additional_forward_args=all_inputs,\n",
    "            n_steps=n_steps,\n",
    "            method=method,\n",
    "            internal_batch_size=internal_batch_size,\n",
    "            return_convergence_delta=False,\n",
    "        )\n",
    "\n",
    "        # handle multiple outputs\n",
    "        output: List[Tuple[Tensor, ...]] = [\n",
    "            tuple(\n",
    "                attributions[\n",
    "                    int(num_outputs_cumsum[i]) : int(num_outputs_cumsum[i + 1])\n",
    "                ]\n",
    "            )\n",
    "            for i in range(len(num_outputs))\n",
    "        ]\n",
    "\n",
    "        if return_convergence_delta:\n",
    "            start_point, end_point = baselines, inps\n",
    "            # computes approximation error based on the completeness axiom\n",
    "            delta = self.compute_convergence_delta(\n",
    "                attributions,\n",
    "                start_point,\n",
    "                end_point,\n",
    "                additional_forward_args=additional_forward_args,\n",
    "                target=target,\n",
    "            )\n",
    "            return _format_outputs(isinstance(self.layer, list), output), delta\n",
    "        return _format_outputs(isinstance(self.layer, list), output)\n",
    "\n",
    "\n",
    "    def has_convergence_delta(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def multiplies_by_inputs(self):\n",
    "        return self.ig.multiplies_by_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_predict_one_proba(text, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, add_special_tokens=True, return_tensors='pt').to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "        # case when cpm returns multiple predictions, take the first\n",
    "        # (corresponding to overall sentiment)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits[0]\n",
    "        preds = F.softmax(logits, dim=1)\n",
    "    model.train()\n",
    "    return preds.squeeze(0)\n",
    "\n",
    "\n",
    "def hf_ig_encodings(text, tokenizer):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    base_ids = [pad_id] * len(input_ids)\n",
    "    input_ids = [cls_id] + input_ids + [sep_id]\n",
    "    base_ids = [cls_id] + base_ids + [sep_id]\n",
    "    return torch.LongTensor([input_ids]), torch.LongTensor([base_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECTS = ['ambiance', 'food', 'noise', 'service']\n",
    "asp_to_ind = {\n",
    "    'overall': None,\n",
    "    **({a: i for i, a in enumerate(ASPECTS)})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_ig_analyses(inputs, targets, tokenizer, model):\n",
    "    data = []\n",
    "    for text, true_class in zip(inputs, targets):\n",
    "        # create visualization for each aspect \n",
    "        for aspect, _ in enumerate(ASPECTS):\n",
    "            score_vis = hf_ig_analysis_one(text, true_class, tokenizer, model, aspect=aspect)\n",
    "            data.append(score_vis)\n",
    "    html = viz.visualize_text(data)\n",
    "\n",
    "def hf_ig_analyses_new(inputs, targets, tokenizer, model):\n",
    "    data = []\n",
    "    for text in inputs:\n",
    "        # create visualization for each aspect \n",
    "        for aspect, _ in enumerate(ASPECTS):\n",
    "            score_vis = hf_ig_analyses_multiple(text, targets, tokenizer, model, aspect=aspect)\n",
    "            data.append(score_vis)\n",
    "    html = viz.visualize_text(data)\n",
    "    \n",
    "def hf_ig_analyses_multiple(text, true_classes, tokenizer, model, device=0, aspect=None):\n",
    "    \n",
    "    scores_for_true_classes = []\n",
    "    for true_class in true_classes:\n",
    "        # Option to look at different layers:\n",
    "        # layer = model.roberta.encoder.layer[0]\n",
    "        # layer = model.roberta.embeddings.word_embeddings\n",
    "        layer = model.bert.embeddings\n",
    "\n",
    "        def ig_forward(inputs):\n",
    "            # logits at first index, cpm returns multiple logits\n",
    "            # logits = model(inputs).logits\n",
    "            # return logits[pred_ind] if isinstance(logits, tuple) else logits\n",
    "            return model(inputs, output_hidden_states=True)\n",
    "\n",
    "        ig = LayerIntegratedGradients(ig_forward, layer)\n",
    "\n",
    "        input_ids, base_ids = hf_ig_encodings(text, tokenizer)\n",
    "\n",
    "        attrs = ig.attribute(\n",
    "            input_ids.to(model.device),\n",
    "            base_ids.to(model.device),\n",
    "            target=true_class,\n",
    "            return_convergence_delta=False,\n",
    "            attribute_to_layer_input=False,\n",
    "            aspect=aspect\n",
    "        )\n",
    "\n",
    "        # Summarize and z-score normalize the attributions\n",
    "        # for each representation in `layer`:\n",
    "        scores = attrs.sum(dim=-1).squeeze(0)\n",
    "        scores_for_true_classes += [scores]\n",
    "    scores_for_true_classes = torch.stack(scores_for_true_classes, dim=0).sum(dim=0)\n",
    "    scores = scores_for_true_classes\n",
    "    scores = (scores - scores.mean()) / scores.norm()\n",
    "\n",
    "    # Intuitive tokens to help with analysis:\n",
    "    raw_input = tokenizer.convert_ids_to_tokens(input_ids.tolist()[0])\n",
    "    # RoBERTa-specific clean-up:\n",
    "    # raw_input = [x.strip(\"Ġ\") for x in raw_input]\n",
    "\n",
    "    # Predictions for comparisons:\n",
    "    pred_probs = hf_predict_one_proba(text, tokenizer, model)\n",
    "    pred_class = pred_probs.argmax()\n",
    "\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "        word_attributions=scores,\n",
    "        pred_prob=pred_probs.max(),\n",
    "        pred_class=pred_class,\n",
    "        true_class=true_class,\n",
    "        # override attribution label with aspect label, to make the visualization clearer\n",
    "        attr_class=ASPECTS[aspect] if aspect is not None else None,\n",
    "        attr_score=attrs.sum(),\n",
    "        raw_input_ids=raw_input,\n",
    "        convergence_score=None)\n",
    "    \n",
    "    max_score = max([abs(score) for score in scores.tolist()])\n",
    "    mag = 1.0/max_score\n",
    "    word_idx = 0\n",
    "    latex_str = \"\"\n",
    "    for score in scores.tolist():\n",
    "        sign = \"green\" if score > 0 else \"red\"\n",
    "        color_mag = int(abs(score*mag)*100)\n",
    "        word = raw_input[word_idx]\n",
    "        latex_str += \"\\\\colorbox{\"+sign+\"!\"+str(color_mag)+\"}{\\\\strut \"+word+\"}\"\n",
    "        word_idx += 1\n",
    "        \n",
    "    print(latex_str)\n",
    "    print()\n",
    "    \n",
    "    return score_vis\n",
    "    \n",
    "def hf_ig_analysis_one(text, true_class, tokenizer, model, device=0, aspect=None):\n",
    "    # Option to look at different layers:\n",
    "    # layer = model.roberta.encoder.layer[0]\n",
    "    # layer = model.roberta.embeddings.word_embeddings\n",
    "    layer = model.bert.embeddings\n",
    "\n",
    "    def ig_forward(inputs):\n",
    "        # logits at first index, cpm returns multiple logits\n",
    "        # logits = model(inputs).logits\n",
    "        # return logits[pred_ind] if isinstance(logits, tuple) else logits\n",
    "        return model(inputs, output_hidden_states=True)\n",
    "\n",
    "    ig = LayerIntegratedGradients(ig_forward, layer)\n",
    "\n",
    "    input_ids, base_ids = hf_ig_encodings(text, tokenizer)\n",
    "\n",
    "    attrs = ig.attribute(\n",
    "        input_ids.to(model.device),\n",
    "        base_ids.to(model.device),\n",
    "        target=true_class,\n",
    "        return_convergence_delta=False,\n",
    "        attribute_to_layer_input=False,\n",
    "        aspect=aspect\n",
    "    )\n",
    "\n",
    "    # Summarize and z-score normalize the attributions\n",
    "    # for each representation in `layer`:\n",
    "    scores = attrs.sum(dim=-1).squeeze(0)\n",
    "    scores = (scores - scores.mean()) / scores.norm()\n",
    "    \n",
    "    # Intuitive tokens to help with analysis:\n",
    "    raw_input = tokenizer.convert_ids_to_tokens(input_ids.tolist()[0])\n",
    "    # RoBERTa-specific clean-up:\n",
    "    # raw_input = [x.strip(\"Ġ\") for x in raw_input]\n",
    "\n",
    "    # Predictions for comparisons:\n",
    "    pred_probs = hf_predict_one_proba(text, tokenizer, model)\n",
    "    pred_class = pred_probs.argmax()\n",
    "\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "        word_attributions=scores,\n",
    "        pred_prob=pred_probs.max(),\n",
    "        pred_class=pred_class,\n",
    "        true_class=true_class,\n",
    "        # override attribution label with aspect label, to make the visualization clearer\n",
    "        attr_class=ASPECTS[aspect] if aspect is not None else None,\n",
    "        attr_score=attrs.sum(),\n",
    "        raw_input_ids=raw_input,\n",
    "        convergence_score=None)\n",
    "    \n",
    "    max_score = max([abs(score) for score in scores.tolist()])\n",
    "    mag = 1.0/max_score\n",
    "    word_idx = 0\n",
    "    latex_str = \"\"\n",
    "    for score in scores.tolist():\n",
    "        sign = \"green\" if score > 0 else \"red\"\n",
    "        color_mag = int(abs(score*mag)*100)\n",
    "        word = raw_input[word_idx]\n",
    "        latex_str += \"\\\\colorbox{\"+sign+\"!\"+str(color_mag)+\"}{\\\\strut \"+word+\"}\"\n",
    "        word_idx += 1\n",
    "        \n",
    "    print(latex_str)\n",
    "    print()\n",
    "    \n",
    "    return score_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "eval_split=permutations_dicts[i][\"eval_split\"]\n",
    "seed=permutations_dicts[i][\"seed\"]\n",
    "class_num=permutations_dicts[i][\"class_num\"]\n",
    "alpha=permutations_dicts[i][\"alpha\"]\n",
    "beta=permutations_dicts[i][\"beta\"]\n",
    "gemma=permutations_dicts[i][\"gemma\"]\n",
    "h_dim=permutations_dicts[i][\"h_dim\"]\n",
    "dataset_type = f'{class_num}-way'\n",
    "control=permutations_dicts[i][\"control\"]\n",
    "model_arch=permutations_dicts[i][\"model_arch\"]\n",
    "k=permutations_dicts[i][\"k\"]\n",
    "interchange_layer=permutations_dicts[i][\"interchange_layer\"]\n",
    "lr=permutations_dicts[i][\"lr\"]\n",
    "counterfactual_type=permutations_dicts[i][\"counterfactual_type\"]\n",
    "\n",
    "if model_arch == \"bert-base-uncased\":\n",
    "    model_path = \"BERT\"\n",
    "    model_module = BERTForCEBaB\n",
    "    explainer_module = CausalProxyModelForBERT\n",
    "elif model_arch == \"roberta-base\":\n",
    "    model_path = \"RoBERTa\" \n",
    "    model_module = RoBERTaForCEBaB\n",
    "    explainer_module = CausalProxyModelForRoBERTa\n",
    "elif model_arch == \"gpt2\":\n",
    "    model_path = \"gpt2\"\n",
    "    model_module = GPT2ForCEBaB\n",
    "    explainer_module = CausalProxyModelForGPT2\n",
    "elif model_arch == \"lstm\":\n",
    "    model_path = \"lstm\"\n",
    "    model_module = LSTMForCEBaB\n",
    "    explainer_module = CausalProxyModelForLSTM\n",
    "model_path += f\"-{control}\"\n",
    "\n",
    "j = 5\n",
    "grid_conditions=(\n",
    "    (\"eval_split\", eval_split),\n",
    "    (\"control\", control),\n",
    "    (\"seed\", seed),\n",
    "    (\"h_dim\", h_dim),\n",
    "    (\"interchange_layer\", interchange_layer),\n",
    "    (\"class_num\", class_num),\n",
    "    (\"k\", k),\n",
    "    (\"alpha\", alpha),\n",
    "    (\"beta\", beta),\n",
    "    (\"gemma\", gemma),\n",
    "    (\"model_arch\", model_arch),\n",
    "    (\"lr\", lr),\n",
    "    (\"counterfactual_type\", counterfactual_type),\n",
    "    (\"checkpoint\", j)\n",
    ")\n",
    "print(\"Running for this setting: \", grid_conditions)\n",
    "\n",
    "blackbox_model_path = f'CEBaB/{model_arch}.CEBaB.sa.'\\\n",
    "                      f'{class_num}-class.exclusive.seed_{seed}'\n",
    "cpm_model_path = f'../proxy_training_results/{model_path}/'\\\n",
    "                 f'cebab.alpha.{alpha}.beta.{beta}.gemma.{gemma}.'\\\n",
    "                 f'lr.{lr}.dim.{h_dim}.hightype.{model_arch}.'\\\n",
    "                 f'CEBaB.cls.dropout.0.1.enc.dropout.0.1.counter.type.'\\\n",
    "                 f'{counterfactual_type}.k.{k}.int.layer.{interchange_layer}.'\\\n",
    "                 f'seed_{seed}/checkpoint-{j}'\n",
    "\n",
    "# load data from HF\n",
    "cebab = datasets.load_dataset(\n",
    "    'CEBaB/CEBaB', use_auth_token=True,\n",
    "    cache_dir=\"../train_cache/\"\n",
    ")\n",
    "\n",
    "train, dev, test = preprocess_hf_dataset_inclusive(\n",
    "    cebab, verbose=1, dataset_type=dataset_type\n",
    ")\n",
    "\n",
    "eval_dataset = dev if eval_split == 'dev' else test\n",
    "\n",
    "tf_model = model_module(\n",
    "    blackbox_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "explainer = explainer_module(\n",
    "    blackbox_model_path,\n",
    "    cpm_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size,\n",
    "    intervention_h_dim=h_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    'The music was too loud, and the decorations were tasteless, but they had friendly waiters and delicious pasta'\n",
    "]\n",
    "\n",
    "labels = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL = False\n",
    "_ = random.seed(seed)\n",
    "_ = np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "hf_ig_analyses_new(\n",
    "    example_sentences, labels, explainer.tokenizer, \n",
    "    explainer.cpm_model.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
