{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file loads the proxy model trained with IIT, and evaluate it for concept-based explanations with the CEBaB dataset.\n",
    "This file tends to follow the one in our OpenTable repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    ")\n",
    "from math import ceil\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from models.modelings_roberta import *\n",
    "from models.modelings_bert import *\n",
    "\n",
    "from eval_pipeline.models.abstract_model import Model \n",
    "from eval_pipeline.explainers.abstract_explainer import Explainer\n",
    "from eval_pipeline.utils.data_utils import preprocess_hf_dataset\n",
    "from eval_pipeline.customized_models.bert import BertForNonlinearSequenceClassification\n",
    "from eval_pipeline.utils import metric_utils, get_intervention_pairs\n",
    "from eval_pipeline.explainers.random_explainer import RandomExplainer\n",
    "from eval_pipeline.explainers.conexp import CONEXP\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set random seeds.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def query_with_aspect_label(\n",
    "    df,\n",
    "    ambiance=\"Positive\",\n",
    "    service=\"Positive\",\n",
    "    noise=\"Positive\",\n",
    "    food=\"Positive\",\n",
    "):\n",
    "    return df[\n",
    "        (df[\"ambiance_aspect_majority\"]==ambiance)&\n",
    "        (df[\"service_aspect_majority\"]==service)&\n",
    "        (df[\"noise_aspect_majority\"]==noise)&\n",
    "        (df[\"food_aspect_majority\"]==food)\n",
    "    ]\n",
    "\n",
    "def flatten_logits(\n",
    "    df,\n",
    "    col_names,\n",
    "):\n",
    "    flatten_list = []\n",
    "    for col_name in col_names:\n",
    "        for row in df[col_name]:\n",
    "            new_row = [v for v in row]\n",
    "            new_row.append(col_name)\n",
    "            flatten_list.append(new_row)\n",
    "    col_names = [f\"feature_{i}\" for i in range(len(flatten_list[-1])-1)]\n",
    "    col_names.append(\"type\")\n",
    "    df = pd.DataFrame(\n",
    "        flatten_list, \n",
    "        columns=col_names\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForCEBaB(Model):\n",
    "    def __init__(self, model_path, device='cpu', batch_size=64):\n",
    "        self.device = device\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = model_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = BertForNonlinearSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            cache_dir=\"../huggingface_cache\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.model_path.split('/')[-1]\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        x = self.tokenizer(df['description'].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
    "        y = df['review_majority'].astype(int)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # assume model was already trained\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, dataset):\n",
    "        self.model.eval()\n",
    "\n",
    "        x, y = self.preprocess(dataset)\n",
    "\n",
    "        # get the predictions batch per batch\n",
    "        probas = []\n",
    "        for i in range(ceil(len(dataset) / self.batch_size)):\n",
    "            x_batch = {k: v[i * self.batch_size:(i + 1) * self.batch_size].to(self.device) for k, v in x.items()}\n",
    "            probas.append(torch.nn.functional.softmax(self.model(**x_batch).logits.cpu(), dim=-1).detach())\n",
    "\n",
    "        probas = torch.concat(probas)\n",
    "        probas = np.round(probas.numpy(), decimals=16)\n",
    "\n",
    "        predictions = np.argmax(probas, axis=1)\n",
    "        clf_report = classification_report(y.to_numpy(), predictions, output_dict=True)\n",
    "\n",
    "        return probas, clf_report\n",
    "\n",
    "    def get_embeddings(self, sentences_list):\n",
    "        x = self.tokenizer(sentences_list, padding=True, truncation=True, return_tensors='pt')\n",
    "        embeddings = []\n",
    "        for i in range(ceil(len(x['input_ids']) / self.batch_size)):\n",
    "            x_batch = {k: v[i * self.batch_size:(i + 1) * self.batch_size].to(self.device) for k, v in x.items()}\n",
    "            embeddings.append(self.model.base_model(**x_batch).pooler_output.detach().cpu().tolist())\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_classification_head(self):\n",
    "        return self.model.classifier\n",
    "    \n",
    "def get_iit_examples(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe in the new data scheme, return all intervention pairs.\n",
    "    \"\"\"\n",
    "    # Drop label distribution and worker information.\n",
    "    columns_to_keep = ['id', 'original_id', 'edit_id', 'is_original', 'edit_goal', 'edit_type', 'description', 'review_majority','food_aspect_majority', 'ambiance_aspect_majority', 'service_aspect_majority', 'noise_aspect_majority']\n",
    "    columns_to_keep += [col for col in df.columns if 'prediction' in col]\n",
    "    df = df[columns_to_keep]\n",
    "    return df\n",
    "\n",
    "class CausalProxyModel(Explainer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        blackbox_model_path,\n",
    "        cpm_model_path, \n",
    "        device, batch_size, \n",
    "        intervention_h_dim=1,\n",
    "        min_iit_pair_examples=1,\n",
    "        match_non_int_type=False,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.min_iit_pair_examples = min_iit_pair_examples\n",
    "        self.match_non_int_type = match_non_int_type\n",
    "        # blackbox model loading.\n",
    "        self.blackbox_model = BertForNonlinearSequenceClassification.from_pretrained(\n",
    "            blackbox_model_path,\n",
    "            cache_dir=\"../huggingface_cache\"\n",
    "        )\n",
    "        self.blackbox_model.to(device)\n",
    "        \n",
    "        # causal proxy model loading.\n",
    "        cpm_config = AutoConfig.from_pretrained(\n",
    "            cpm_model_path,\n",
    "            cache_dir=\"../huggingface_cache/\",\n",
    "            use_auth_token=True if \"CEBaB/\" in model_path else False,\n",
    "        )\n",
    "        try:\n",
    "            cpm_config.intervention_h_dim = cpm_config.intervention_h_dim\n",
    "        except:\n",
    "            cpm_config.intervention_h_dim = intervention_h_dim\n",
    "        print(f\"intervention_h_dim={cpm_config.intervention_h_dim}\")\n",
    "        cpm_model = IITBERTForSequenceClassification.from_pretrained(\n",
    "            cpm_model_path,\n",
    "            config=cpm_config,\n",
    "            cache_dir=\"../huggingface_cache/\"\n",
    "        )\n",
    "        cpm_model.to(device)\n",
    "        self.cpm_model = InterventionableIITTransformerForSequenceClassification(\n",
    "            model=cpm_model\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "    def fit(self, dataset, classifier_predictions, classifier, dev_dataset=None):\n",
    "        # we don't need to train IIT here.\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, pairs_dataset, dev_dataset):\n",
    "        \n",
    "        # configs\n",
    "        min_iit_pair_examples = self.min_iit_pair_examples\n",
    "        match_non_int_type = self.match_non_int_type\n",
    "        \n",
    "        query_dataset = get_iit_examples(dev_dataset)\n",
    "        iit_pairs_dataset = []\n",
    "        iit_id = 0\n",
    "        for index, row in pairs_dataset.iterrows():\n",
    "            query_description_base = row['description_base']\n",
    "            query_int_type = row['intervention_type']\n",
    "            query_non_int_type = {\n",
    "                \"ambiance\", \"food\", \"noise\", \"service\"\n",
    "            } - {query_int_type}\n",
    "            query_int_aspect_base = row[\"intervention_aspect_base\"]\n",
    "            query_int_aspect_assignment = row['intervention_aspect_counterfactual']\n",
    "            query_original_id = row[\"original_id_base\"]\n",
    "            matched_iit_examples = query_dataset[\n",
    "                (query_dataset[f\"{query_int_type}_aspect_majority\"]==query_int_aspect_assignment)&\n",
    "                (query_dataset[\"original_id\"]!=query_original_id)\n",
    "            ]\n",
    "            if match_non_int_type:\n",
    "                for _t in query_non_int_type:\n",
    "                    matched_iit_examples = matched_iit_examples[\n",
    "                        (matched_iit_examples[f\"{_t}_aspect_majority\"]==\\\n",
    "                         row[f\"{_t}_aspect_majority_base\"])\n",
    "                    ]\n",
    "            if len(set(matched_iit_examples[\"id\"])) < min_iit_pair_examples:\n",
    "                if match_non_int_type:\n",
    "                    # simply avoid mapping the rest of the aspects.\n",
    "                    matched_iit_examples = query_dataset[\n",
    "                        (query_dataset[f\"{query_int_type}_aspect_majority\"]==query_int_aspect_assignment)&\n",
    "                        (query_dataset[\"original_id\"]!=query_original_id)\n",
    "                    ]\n",
    "                else:\n",
    "                    assert False # we need to check the number!\n",
    "            sampled_iit_example_ids = random.sample(\n",
    "                set(matched_iit_examples[\"id\"]), min_iit_pair_examples\n",
    "            )\n",
    "            for _id in sampled_iit_example_ids:\n",
    "                description_iit = query_dataset[query_dataset[\"id\"]==_id][\"description\"].iloc[0]\n",
    "                iit_pairs_dataset += [[\n",
    "                    iit_id,\n",
    "                    query_int_type,\n",
    "                    query_description_base, \n",
    "                    description_iit\n",
    "                ]]\n",
    "            iit_id += 1\n",
    "        iit_pairs_dataset = pd.DataFrame(\n",
    "            columns=[\n",
    "                'iit_id',\n",
    "                'intervention_type', \n",
    "                'description_base', \n",
    "                'description_iit'], \n",
    "            data=iit_pairs_dataset\n",
    "        )\n",
    "        \n",
    "        base_x = self.tokenizer(\n",
    "            iit_pairs_dataset['description_base'].to_list(), \n",
    "            padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        source_x = self.tokenizer(\n",
    "            iit_pairs_dataset['description_iit'].to_list(), \n",
    "            padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        intervention_corr = []\n",
    "        for _type in iit_pairs_dataset[\"intervention_type\"].tolist():\n",
    "            if _type == \"ambiance\":\n",
    "                intervention_corr += [0]\n",
    "            if _type == \"food\":\n",
    "                intervention_corr += [1]\n",
    "            if _type == \"noise\":\n",
    "                intervention_corr += [2]\n",
    "            if _type == \"service\":\n",
    "                intervention_corr += [3]\n",
    "        intervention_corr = torch.tensor(intervention_corr).long()\n",
    "        return base_x, source_x, intervention_corr, iit_pairs_dataset\n",
    "    \n",
    "    def estimate_icace(self, pairs, df):\n",
    "        CPM_iTEs = []\n",
    "        self.blackbox_model.eval()\n",
    "        self.cpm_model.model.eval()\n",
    "        base_x, source_x, intervention_corr, iit_pairs_dataset = self.preprocess(\n",
    "            pairs, df\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(ceil(len(iit_pairs_dataset)/self.batch_size))):\n",
    "                base_x_batch = {k:v[i*self.batch_size:(i+1)*self.batch_size].to(self.device) for k,v in base_x.items()} \n",
    "                source_x_batch = {k:v[i*self.batch_size:(i+1)*self.batch_size].to(self.device) for k,v in source_x.items()} \n",
    "                intervention_corr_batch = intervention_corr[i*self.batch_size:(i+1)*self.batch_size].to(self.device)\n",
    "                \n",
    "                base_outputs = torch.nn.functional.softmax(\n",
    "                    self.blackbox_model(**base_x_batch).logits.cpu(), dim=-1\n",
    "                ).detach()\n",
    "                _, _, counterfactual_outputs = self.cpm_model.forward(\n",
    "                    base=(base_x_batch['input_ids'], base_x_batch['attention_mask']),\n",
    "                    source=(source_x_batch['input_ids'], source_x_batch['attention_mask']),\n",
    "                    base_intervention_corr=intervention_corr_batch,\n",
    "                    source_intervention_corr=intervention_corr_batch,\n",
    "                )\n",
    "                counterfactual_outputs = torch.nn.functional.softmax(\n",
    "                    counterfactual_outputs[\"logits\"][0].cpu(), dim=-1\n",
    "                ).detach()\n",
    "                CPM_iTE = counterfactual_outputs-base_outputs\n",
    "                CPM_iTEs.append(CPM_iTE)\n",
    "        CPM_iTEs = torch.concat(CPM_iTEs)\n",
    "        CPM_iTEs = np.round(CPM_iTEs.numpy(), decimals=4)\n",
    "\n",
    "        # only for iit explainer!\n",
    "        iit_pairs_dataset[\"EiCaCE\"] = list(CPM_iTEs)\n",
    "        CPM_iTEs = list(iit_pairs_dataset.groupby([\"iit_id\"])[\"EiCaCE\"].mean())\n",
    "        \n",
    "        return CPM_iTEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cebab_pipeline(\n",
    "    model, explainer, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type='5-way', \n",
    "    shorten_model_name=False,\n",
    "    correction_epsilon=0.001,\n",
    "):\n",
    "    # get predictions on train and dev\n",
    "    train_predictions, _ = model.predict_proba(\n",
    "        train_dataset\n",
    "    )\n",
    "    dev_predictions, dev_report = model.predict_proba(\n",
    "        dev_dataset\n",
    "    )\n",
    "\n",
    "    # append predictions to datasets\n",
    "    train_dataset['prediction'] = list(train_predictions)\n",
    "    dev_dataset['prediction'] = list(dev_predictions)\n",
    "\n",
    "    # fit explainer\n",
    "    explainer.fit(\n",
    "        train_dataset, train_predictions, \n",
    "        model, dev_dataset\n",
    "    )\n",
    "\n",
    "    # get intervention pairs\n",
    "    \n",
    "    pairs_dataset = get_intervention_pairs(\n",
    "        dev_dataset, dataset_type=dataset_type\n",
    "    )  # TODO why is the index not unique here?\n",
    "        \n",
    "    # get explanations\n",
    "    if isinstance(explanator, CausalProxyModel):\n",
    "        explanations = explainer.estimate_icace(\n",
    "            pairs_dataset,\n",
    "            train_dataset # for query data.\n",
    "        )\n",
    "    else:\n",
    "        explanations = explainer.estimate_icace(\n",
    "            pairs_dataset,\n",
    "        )\n",
    "    \n",
    "    # append explanations to the pairs\n",
    "    pairs_dataset['EICaCE'] = explanations\n",
    "    \n",
    "    # TODO: add cosine\n",
    "    pairs_dataset = metric_utils._calculate_ite(pairs_dataset)  # effect of crowd-workers on other crowd-workers (no model, no explainer)\n",
    "    \n",
    "    def _calculate_icace(pairs):\n",
    "        \"\"\"\n",
    "        This metric measures the effect of a certain concept on the given model.\n",
    "        It is independent of the explainer.\n",
    "        \"\"\"\n",
    "        pairs['ICaCE'] = (pairs['prediction_counterfactual'] - pairs['prediction_base']).apply(lambda x: np.round(x, decimals=4))\n",
    "\n",
    "        return pairs\n",
    "    pairs_dataset = _calculate_icace(pairs_dataset)  # effect of concept on the model (with model, no explainer)\n",
    "    \n",
    "    # TOREMOVE: just to try if we ignore all [0,0,...] cases here.\n",
    "    # zeros_like = tuple([0.0 for i in range(int(dataset_type.split(\"-\")[0]))])\n",
    "    # pairs_dataset = pairs_dataset[pairs_dataset.ICaCE.map(tuple).isin([zeros_like])==False]\n",
    "    \n",
    "    def _cosine_distance(a,b,epsilon):\n",
    "        if epsilon == None:\n",
    "            if np.linalg.norm(a, ord=2) == 0 or np.linalg.norm(b, ord=2) == 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return cosine(a,b)\n",
    "        \n",
    "        if np.linalg.norm(a, ord=2) == 0 and np.linalg.norm(b, ord=2) == 0:\n",
    "            \"\"\"\n",
    "            We cannot determine whether prediction is corrected or not.\n",
    "            Thus, we simply return 1.\n",
    "            \"\"\"\n",
    "            return 1\n",
    "        elif np.linalg.norm(a, ord=2) == 0:\n",
    "            \"\"\"\n",
    "            When true iCACE score is 0, instead of always returning 1, we\n",
    "            allow some epsilon error by default. If the EiCACE is within a\n",
    "            range, we return the error as 0.\n",
    "            \"\"\"\n",
    "            if np.max(np.abs(b)) <= epsilon:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        elif np.linalg.norm(b, ord=2) == 0:\n",
    "            \"\"\"\n",
    "            This case happens when iCACE is not 0, but EiCACE is 0. This is\n",
    "            unlikely, but we give score of 1 for this case.\n",
    "            \"\"\"\n",
    "            return 1\n",
    "        else:\n",
    "            return cosine(a,b)\n",
    "    \n",
    "    def _calculate_estimate_loss(pairs,epsilon):\n",
    "        \"\"\"\n",
    "        Calculate the distance between the ICaCE and EICaCE.\n",
    "        \"\"\"\n",
    "\n",
    "        pairs['ICaCE-L2'] = pairs[['ICaCE', 'EICaCE']].apply(lambda x: np.linalg.norm(x[0] - x[1], ord=2), axis=1)\n",
    "        pairs['ICaCE-cosine'] = pairs[['ICaCE', 'EICaCE']].apply(lambda x: _cosine_distance(x[0], x[1], epsilon), axis=1)\n",
    "        pairs['ICaCE-normdiff'] = pairs[['ICaCE', 'EICaCE']].apply(lambda x: abs(np.linalg.norm(x[0], ord=2) - np.linalg.norm(x[1], ord=2)), axis=1)\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "    pairs_dataset = _calculate_estimate_loss(pairs_dataset,correction_epsilon)  # l2 CEBaB Score (model and explainer)\n",
    "\n",
    "    # only keep columns relevant for metrics\n",
    "    CEBaB_metrics_per_pair = pairs_dataset[[\n",
    "        'intervention_type', 'intervention_aspect_base', 'intervention_aspect_counterfactual', 'ITE', 'ICaCE', 'EICaCE', 'ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff']].copy()\n",
    "    CEBaB_metrics_per_pair['count'] = 1\n",
    "\n",
    "    # get CEBaB tables\n",
    "    metrics = ['count', 'ICaCE', 'EICaCE']\n",
    "\n",
    "    groupby_aspect_direction = ['intervention_type', 'intervention_aspect_base', 'intervention_aspect_counterfactual']\n",
    "\n",
    "    CaCE_per_aspect_direction = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, metrics)\n",
    "    CaCE_per_aspect_direction.columns = ['count', 'CaCE', 'ECaCE']\n",
    "    CaCE_per_aspect_direction = CaCE_per_aspect_direction.set_index(['count'], append=True)\n",
    "    \n",
    "    ACaCE_per_aspect = metric_utils._aggregate_metrics(CaCE_per_aspect_direction.abs(), ['intervention_type'], ['CaCE', 'ECaCE'])\n",
    "    ACaCE_per_aspect.columns = ['ACaCE', 'EACaCE']\n",
    "\n",
    "    CEBaB_metrics_per_aspect_direction = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, ['count', 'ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff'])\n",
    "    CEBaB_metrics_per_aspect_direction.columns = ['count', 'ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff']\n",
    "    CEBaB_metrics_per_aspect_direction = CEBaB_metrics_per_aspect_direction.set_index(['count'], append=True)\n",
    "\n",
    "    CEBaB_metrics_per_aspect = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, ['intervention_type'], ['count', 'ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff'])\n",
    "    CEBaB_metrics_per_aspect.columns = ['count', 'ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff']\n",
    "    CEBaB_metrics_per_aspect = CEBaB_metrics_per_aspect.set_index(['count'], append=True)\n",
    "\n",
    "    CEBaB_metrics = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, [], ['ICaCE-L2', 'ICaCE-cosine', 'ICaCE-normdiff'])\n",
    "\n",
    "    # get ATE table\n",
    "    ATE = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, ['count', 'ITE'])\n",
    "    ATE.columns = ['count', 'ATE']\n",
    "\n",
    "    # add model and explainer information\n",
    "    if shorten_model_name:\n",
    "        model_name = str(model).split('.')[0]\n",
    "    else:\n",
    "        model_name = str(model)\n",
    "\n",
    "    CaCE_per_aspect_direction.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) if col != 'CaCE' else (model_name, '', col) for col in CaCE_per_aspect_direction.columns])\n",
    "    ACaCE_per_aspect.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) if col != 'ACaCE' else (model_name, '', col) for col in ACaCE_per_aspect.columns])\n",
    "    CEBaB_metrics_per_aspect_direction.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) for col in CEBaB_metrics_per_aspect_direction.columns])\n",
    "    CEBaB_metrics_per_aspect.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) for col in CEBaB_metrics_per_aspect.columns])\n",
    "    CEBaB_metrics.index = pd.MultiIndex.from_product([[model_name], [str(explainer)], CEBaB_metrics.index])\n",
    "    \n",
    "    # performance report\n",
    "    performance_report_index = ['macro-f1', 'accuracy']\n",
    "    performance_report_data = [dev_report['macro avg']['f1-score'], dev_report['accuracy']]\n",
    "    performance_report_col = [model_name]\n",
    "    performance_report = pd.DataFrame(data=performance_report_data, index=performance_report_index, columns=performance_report_col)\n",
    "\n",
    "    return pairs_dataset, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, ACaCE_per_aspect, performance_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mannual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "class_num=2\n",
    "gemma=3.0\n",
    "h_dim=192\n",
    "dataset_type = f'{class_num}-way'\n",
    "control=True\n",
    "correction_epsilon=1e-0\n",
    "cls_dropout=None\n",
    "enc_dropout=None\n",
    "\n",
    "blackbox_model_path = f'CEBaB/bert-base-uncased.CEBaB.sa.{class_num}-class.exclusive.seed_{seed}'\n",
    "if control:\n",
    "    cpm_model_path = blackbox_model_path\n",
    "else:\n",
    "    if cls_dropout != None and enc_dropout != None:\n",
    "        cpm_model_path = f'./proxy_training_results/'\\\n",
    "                           f'cebab.train.train.alpha.1.0'\\\n",
    "                           f'.beta.1.0.gemma.{gemma}.dim.{h_dim}.hightype.'\\\n",
    "                           f'bert-base-uncased.Proxy.'\\\n",
    "                           f'CEBaB.sa.{class_num}-class.exclusive.'\\\n",
    "                           f'mode.align.cls.dropout.{cls_dropout}.enc.dropout.{enc_dropout}.seed_{seed}'\n",
    "    elif cls_dropout != None:\n",
    "        cpm_model_path = f'./proxy_training_results/'\\\n",
    "                           f'cebab.train.train.alpha.1.0'\\\n",
    "                           f'.beta.1.0.gemma.{gemma}.dim.{h_dim}.hightype.'\\\n",
    "                           f'bert-base-uncased.Proxy.'\\\n",
    "                           f'CEBaB.sa.{class_num}-class.exclusive.'\\\n",
    "                           f'mode.align.dropout.{cls_dropout}.seed_{seed}'\n",
    "    else:\n",
    "        cpm_model_path = f'./proxy_training_results/'\\\n",
    "                           f'cebab.train.train.alpha.1.0'\\\n",
    "                           f'.beta.1.0.gemma.{gemma}.dim.{h_dim}.hightype.'\\\n",
    "                           f'bert-base-uncased.Proxy.'\\\n",
    "                           f'CEBaB.sa.{class_num}-class.exclusive.'\\\n",
    "                           f'mode.align.seed_{seed}'\n",
    "\n",
    "device = 'cuda:8'\n",
    "batch_size = 32\n",
    "\n",
    "# load data from HF\n",
    "cebab = datasets.load_dataset(\n",
    "    'CEBaB/CEBaB', use_auth_token=True,\n",
    "    cache_dir=\"../huggingface_cache/\"\n",
    ")\n",
    "cebab['train'] = cebab['train_exclusive']\n",
    "train, dev, test = preprocess_hf_dataset(\n",
    "    cebab, one_example_per_world=False, \n",
    "    verbose=1, dataset_type=dataset_type\n",
    ")\n",
    "\n",
    "tf_model = BERTForCEBaB(\n",
    "    blackbox_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "explanator = CausalProxyModel(\n",
    "    blackbox_model_path,\n",
    "    cpm_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size,\n",
    "    intervention_h_dim=h_dim,\n",
    ")\n",
    "# explanator = RandomExplainer()\n",
    "# explanator = CONEXP()\n",
    "\n",
    "train_dataset = train.copy()\n",
    "dev_dataset = test.copy()\n",
    "\n",
    "result_per_example, ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, \\\n",
    "CEBaB_metrics_per_aspect, CaCE_per_aspect_direction, \\\n",
    "ACaCE_per_aspect, performance_report = cebab_pipeline(\n",
    "    tf_model, explanator, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type=dataset_type,\n",
    "    correction_epsilon=correction_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Pool Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
