{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file loads the proxy model trained with IIT, and evaluate it for concept-based explanations with the CEBaB dataset.\n",
    "This file tends to follow the one in our OpenTable repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    ")\n",
    "from math import ceil\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from models.modelings_roberta import *\n",
    "from models.modelings_bert import *\n",
    "\n",
    "from eval_pipeline.models.abstract_model import Model \n",
    "from eval_pipeline.explainers.abstract_explainer import Explainer\n",
    "from eval_pipeline.utils.data_utils import preprocess_hf_dataset\n",
    "from eval_pipeline.customized_models.bert import BertForNonlinearSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2958709e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: set random seeds.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForCEBaB(Model):\n",
    "    def __init__(self, model_path, device='cpu', batch_size=64):\n",
    "        self.device = device\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = model_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = BertForNonlinearSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            cache_dir=\"../huggingface_cache\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.model_path.split('/')[-1]\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        x = self.tokenizer(df['description'].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
    "        y = df['review_majority'].astype(int)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # assume model was already trained\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, dataset):\n",
    "        self.model.eval()\n",
    "\n",
    "        x, y = self.preprocess(dataset)\n",
    "\n",
    "        # get the predictions batch per batch\n",
    "        probas = []\n",
    "        for i in range(ceil(len(dataset) / self.batch_size)):\n",
    "            x_batch = {k: v[i * self.batch_size:(i + 1) * self.batch_size].to(self.device) for k, v in x.items()}\n",
    "            probas.append(torch.nn.functional.softmax(self.model(**x_batch).logits.cpu(), dim=-1).detach())\n",
    "\n",
    "        probas = torch.concat(probas)\n",
    "        probas = np.round(probas.numpy(), decimals=4)\n",
    "\n",
    "        predictions = np.argmax(probas, axis=1)\n",
    "        clf_report = classification_report(y.to_numpy(), predictions, output_dict=True)\n",
    "\n",
    "        return probas, clf_report\n",
    "\n",
    "    def get_embeddings(self, sentences_list):\n",
    "        x = self.tokenizer(sentences_list, padding=True, truncation=True, return_tensors='pt')\n",
    "        embeddings = []\n",
    "        for i in range(ceil(len(x['input_ids']) / self.batch_size)):\n",
    "            x_batch = {k: v[i * self.batch_size:(i + 1) * self.batch_size].to(self.device) for k, v in x.items()}\n",
    "            embeddings.append(self.model.base_model(**x_batch).pooler_output.detach().cpu().tolist())\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "def get_iit_examples(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe in the new data scheme, return all intervention pairs.\n",
    "    \"\"\"\n",
    "    # Drop label distribution and worker information.\n",
    "    columns_to_keep = ['id', 'original_id', 'edit_id', 'is_original', 'edit_goal', 'edit_type', 'description', 'review_majority','food_aspect_majority', 'ambiance_aspect_majority', 'service_aspect_majority', 'noise_aspect_majority']\n",
    "    columns_to_keep += [col for col in df.columns if 'prediction' in col]\n",
    "    df = df[columns_to_keep]\n",
    "    return df\n",
    "\n",
    "class ProxyIIT():\n",
    "    def __init__(self, model_path, device, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        model = IITBERTForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            cache_dir=\"../huggingface_cache/\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model.to(device)\n",
    "        self.model = InterventionableIITTransformerForSequenceClassification(\n",
    "            model=model\n",
    "        )\n",
    "    def fit(self, dataset, classifier_predictions, classifier, dev_dataset=None):\n",
    "        # we don't need to train IIT here.\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, pairs_dataset, dev_dataset):\n",
    "\n",
    "        min_iit_pair_examples = 10\n",
    "        query_dataset = get_iit_examples(dev_dataset)\n",
    "        iit_pairs_dataset = []\n",
    "        iit_id = 0\n",
    "        for index, row in pairs_dataset.iterrows():\n",
    "            query_description_base = row['description_base']\n",
    "            query_int_type = row['intervention_type']\n",
    "            query_int_aspect_base = row[\"intervention_aspect_base\"]\n",
    "            query_int_aspect_assignment = row['intervention_aspect_counterfactual']\n",
    "            query_original_id = row[\"original_id_base\"]\n",
    "            matched_iit_examples = query_dataset[\n",
    "                (query_dataset[f\"{query_int_type}_aspect_majority\"]==query_int_aspect_assignment)&\n",
    "                (query_dataset[\"original_id\"]!=query_original_id)\n",
    "            ]\n",
    "            if len(set(matched_iit_examples[\"id\"])) < min_iit_pair_examples:\n",
    "                assert False # we need to check the number!\n",
    "            sampled_iit_example_ids = random.sample(\n",
    "                set(matched_iit_examples[\"id\"]), min_iit_pair_examples\n",
    "            )\n",
    "            for _id in sampled_iit_example_ids:\n",
    "                description_iit = query_dataset[query_dataset[\"id\"]==_id][\"description\"].iloc[0]\n",
    "                iit_pairs_dataset += [[\n",
    "                    iit_id,\n",
    "                    query_int_type, query_int_aspect_base, \n",
    "                    query_int_aspect_assignment, \n",
    "                    query_description_base, description_iit\n",
    "                ]]\n",
    "            iit_id += 1\n",
    "        iit_pairs_dataset = pd.DataFrame(\n",
    "            columns=[\n",
    "                'iit_id',\n",
    "                'intervention_type', \n",
    "                'intervention_aspect_base', \n",
    "                'intervention_aspect_counterfactual',\n",
    "                'description_base', \n",
    "                'description_iit'], \n",
    "            data=iit_pairs_dataset\n",
    "        )\n",
    "        \n",
    "        base_x = self.tokenizer(\n",
    "            iit_pairs_dataset['description_base'].to_list(), \n",
    "            padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        source_x = self.tokenizer(\n",
    "            iit_pairs_dataset['description_iit'].to_list(), \n",
    "            padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        intervention_corr = []\n",
    "        for _type in iit_pairs_dataset[\"intervention_type\"].tolist():\n",
    "            if _type == \"ambiance\":\n",
    "                intervention_corr += [0]\n",
    "            if _type == \"food\":\n",
    "                intervention_corr += [1]\n",
    "            if _type == \"noise\":\n",
    "                intervention_corr += [2]\n",
    "            if _type == \"service\":\n",
    "                intervention_corr += [3]\n",
    "        intervention_corr = torch.tensor(intervention_corr).long()\n",
    "        return base_x, source_x, intervention_corr, iit_pairs_dataset\n",
    "    \n",
    "    def predict_proba(self, pairs, df):\n",
    "        ProxyIIT_iTEs = []\n",
    "        self.model.model.eval()\n",
    "        base_x, source_x, intervention_corr, iit_pairs_dataset = self.preprocess(\n",
    "            pairs, df\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(ceil(len(iit_pairs_dataset)/self.batch_size))):\n",
    "                base_x_batch = {k:v[i*self.batch_size:(i+1)*self.batch_size].to(self.device) for k,v in base_x.items()} \n",
    "                source_x_batch = {k:v[i*self.batch_size:(i+1)*self.batch_size].to(self.device) for k,v in source_x.items()} \n",
    "                intervention_corr_batch = intervention_corr[i*self.batch_size:(i+1)*self.batch_size].to(self.device)\n",
    "                \n",
    "                base_outputs, _, counterfactual_outputs = self.model.forward(\n",
    "                    base=(base_x_batch['input_ids'], base_x_batch['attention_mask']),\n",
    "                    source=(source_x_batch['input_ids'], source_x_batch['attention_mask']),\n",
    "                    base_intervention_corr=intervention_corr_batch,\n",
    "                    source_intervention_corr=intervention_corr_batch,\n",
    "                )\n",
    "                base_outputs = torch.nn.functional.softmax(base_outputs[\"logits\"][0].cpu(), dim=-1).detach()\n",
    "                counterfactual_outputs = torch.nn.functional.softmax(counterfactual_outputs[\"logits\"][0].cpu(), dim=-1).detach()\n",
    "                ProxyIIT_iTE = counterfactual_outputs-base_outputs\n",
    "                ProxyIIT_iTEs.append(ProxyIIT_iTE)\n",
    "        ProxyIIT_iTEs = torch.concat(ProxyIIT_iTEs)\n",
    "        ProxyIIT_iTEs = np.round(ProxyIIT_iTEs.numpy(), decimals=4)\n",
    "\n",
    "        # only for iit explainer!\n",
    "        iit_pairs_dataset[\"EiCaCE\"] = list(ProxyIIT_iTEs)\n",
    "        ProxyIIT_iTEs = list(iit_pairs_dataset.groupby([\"iit_id\"])[\"EiCaCE\"].mean())\n",
    "        \n",
    "        return ProxyIIT_iTEs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from eval_pipeline.utils import metric_utils, get_intervention_pairs\n",
    "\n",
    "def cebab_pipeline(\n",
    "    model, explainer, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type='5-way', \n",
    "    shorten_model_name=False\n",
    "):\n",
    "    # get predictions on train and dev\n",
    "    train_predictions, _ = model.predict_proba(\n",
    "        train_dataset\n",
    "    )\n",
    "    dev_predictions, dev_report = model.predict_proba(\n",
    "        dev_dataset\n",
    "    )\n",
    "\n",
    "    # append predictions to datasets\n",
    "    train_dataset['prediction'] = list(train_predictions)\n",
    "    dev_dataset['prediction'] = list(dev_predictions)\n",
    "\n",
    "    # fit explainer\n",
    "    explainer.fit(\n",
    "        train_dataset, train_predictions, \n",
    "        model, dev_dataset\n",
    "    )\n",
    "\n",
    "    # get intervention pairs\n",
    "    pairs_dataset = get_intervention_pairs(\n",
    "        dev_dataset, dataset_type=dataset_type\n",
    "    )  # TODO why is the index not unique here?\n",
    "     \n",
    "    # get explanations\n",
    "    explanations = explainer.predict_proba(\n",
    "        pairs_dataset,\n",
    "        dev_dataset\n",
    "    )\n",
    "    \n",
    "    # append explanations to the pairs\n",
    "    pairs_dataset['EICaCE'] = explanations\n",
    "    pairs_dataset = metric_utils._calculate_ite(pairs_dataset)  # effect of crowd-workers on other crowd-workers (no model, no explainer)\n",
    "    pairs_dataset = metric_utils._calculate_icace(pairs_dataset)  # effect of concept on the model (with model, no explainer)\n",
    "    pairs_dataset = metric_utils._calculate_estimate_loss(pairs_dataset)  # l2 CEBaB Score (model and explainer)\n",
    "\n",
    "    # only keep columns relevant for metrics\n",
    "    CEBaB_metrics_per_pair = pairs_dataset[[\n",
    "        'intervention_type', 'intervention_aspect_base', 'intervention_aspect_counterfactual', 'ITE', 'ICaCE', 'EICaCE', 'ICaCE-error']].copy()\n",
    "    CEBaB_metrics_per_pair['count'] = 1\n",
    "\n",
    "    # get CEBaB tables\n",
    "    metrics = ['count', 'ICaCE', 'EICaCE']\n",
    "\n",
    "    groupby_aspect_direction = ['intervention_type', 'intervention_aspect_base', 'intervention_aspect_counterfactual']\n",
    "\n",
    "    CaCE_per_aspect_direction = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, metrics)\n",
    "    CaCE_per_aspect_direction.columns = ['count', 'CaCE', 'ECaCE']\n",
    "    CaCE_per_aspect_direction = CaCE_per_aspect_direction.set_index(['count'], append=True)\n",
    "    \n",
    "    ACaCE_per_aspect = metric_utils._aggregate_metrics(CaCE_per_aspect_direction.abs(), ['intervention_type'], ['CaCE', 'ECaCE'])\n",
    "    ACaCE_per_aspect.columns = ['ACaCE', 'EACaCE']\n",
    "\n",
    "    CEBaB_metrics_per_aspect_direction = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, ['count', 'ICaCE-error'])\n",
    "    CEBaB_metrics_per_aspect_direction.columns = ['count', 'ICaCE-error']\n",
    "    CEBaB_metrics_per_aspect_direction = CEBaB_metrics_per_aspect_direction.set_index(['count'], append=True)\n",
    "\n",
    "    CEBaB_metrics = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, [], ['ICaCE-error'])\n",
    "\n",
    "    # get ATE table\n",
    "    ATE = metric_utils._aggregate_metrics(CEBaB_metrics_per_pair, groupby_aspect_direction, ['count', 'ITE'])\n",
    "    ATE.columns = ['count', 'ATE']\n",
    "    # ATE = ATE.set_index(['count'], append=True)  # TODO why is the count a part of the index?\n",
    "\n",
    "    # add model and explainer information\n",
    "    if shorten_model_name:\n",
    "        model_name = str(model).split('.')[0]\n",
    "    else:\n",
    "        model_name = str(model)\n",
    "\n",
    "    CaCE_per_aspect_direction.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) if col != 'CaCE' else (model_name, '', col) for col in CaCE_per_aspect_direction.columns])\n",
    "    ACaCE_per_aspect.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) if col != 'ACaCE' else (model_name, '', col) for col in ACaCE_per_aspect.columns])\n",
    "    CEBaB_metrics_per_aspect_direction.columns = pd.MultiIndex.from_tuples(\n",
    "        [(model_name, str(explainer), col) for col in CEBaB_metrics_per_aspect_direction.columns])\n",
    "    CEBaB_metrics.index = pd.MultiIndex.from_product([[model_name], [str(explainer)], CEBaB_metrics.index])\n",
    "\n",
    "    return ATE, CEBaB_metrics, CEBaB_metrics_per_aspect_direction, CaCE_per_aspect_direction, ACaCE_per_aspect, dev_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration CEBaB--CEBaB-0e2f7ed67c9d7e55\n",
      "Reusing dataset parquet (../huggingface_cache/parquet/CEBaB--CEBaB-0e2f7ed67c9d7e55/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6e0f78c79a4a20b09bc44e7dfc7a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no majority reviews: 16.03% of train dataset.\n",
      "Dropped 2604 examples with a neutral label.\n",
      "Dropped 452 examples with a neutral label.\n",
      "Dropped 461 examples with a neutral label.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'CEBaB/bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_42'\n",
    "proxy_model_path = './proxy_training_results/cebab.train.train.alpha.1.0.beta.1.0.dim.192.hightype.bert-base-uncased.Proxy.CEBaB.sa.2-class.exclusive.mode.align.seed_42'\n",
    "\n",
    "dataset_type = '2-way'\n",
    "\n",
    "device = 'cuda:0'\n",
    "batch_size = 32\n",
    "\n",
    "# load data from HF\n",
    "cebab = datasets.load_dataset(\n",
    "    'CEBaB/CEBaB', use_auth_token=True,\n",
    "    cache_dir=\"../huggingface_cache/\"\n",
    ")\n",
    "cebab['train'] = cebab['train_exclusive']\n",
    "train, dev, test = preprocess_hf_dataset(\n",
    "    cebab, one_example_per_world=False, \n",
    "    verbose=1, dataset_type=dataset_type\n",
    ")\n",
    "\n",
    "tf_model = BERTForCEBaB(\n",
    "    model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "explanator = ProxyIIT(\n",
    "    proxy_model_path, \n",
    "    device=device, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "train_dataset = train.copy()\n",
    "dev_dataset = dev.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [01:47<00:00,  6.58it/s]\n"
     ]
    }
   ],
   "source": [
    "ATE, CEBaB_metrics, \\\n",
    "CEBaB_metrics_per_aspect_direction, \\\n",
    "CaCE_per_aspect_direction, \\\n",
    "ACaCE_per_aspect, dev_report = cebab_pipeline(\n",
    "    tf_model, explanator, \n",
    "    train_dataset, dev_dataset, \n",
    "    dataset_type='2-way'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ICaCE-error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased.CEBaB.sa.2-class.exclusive.seed_42</th>\n",
       "      <th>&lt;__main__.ProxyIIT object at 0x7f281604ffd0&gt;</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.2141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      ICaCE-error\n",
       "bert-base-uncased.CEBaB.sa.2-class.exclusive.se... <__main__.ProxyIIT object at 0x7f281604ffd0> mean       0.2141"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEBaB_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
